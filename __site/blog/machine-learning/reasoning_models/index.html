<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
   <link rel="stylesheet" href="/libs/katex/katex.min.css">
     
   <link rel="stylesheet" href="/libs/highlight/styles/github.min.css">
   
  <link rel="stylesheet" href="/css/franklin.css">
<link rel="stylesheet" href="/css/hypertext.css">
<link rel="icon" href="/assets/favicon.png">

   <title>How Reasoning Models Work</title>  
</head>
<body>
<header>
  <h1 style="color:#283747">computer science</h1>
  <nav>
    <a href="/" class="current">Tags</a>
  | <a href="/blog/" >Notes</a>
  <hr/>
  </nav>
</header>


<!-- Content appended here -->
<div class="franklin-content"><h1 id="how_reasoning_models_work"><a href="#how_reasoning_models_work" class="header-anchor">How Reasoning Models Work</a></h1>
<p>This post builds directly on <a href="/blog/machine-learning/llm_loss/">The Transformer Loss Function</a>. If you haven&#39;t read that, the short version: a transformer predicts the next token at each position by projecting its hidden state to logits over a vocabulary of size \(V\), then minimizing cross-entropy:</p>
\[\mathcal{L} = -\frac{1}{N}\sum_{i=1}^{N} \log P(y_i \mid y_1, \ldots, y_{i-1}; \theta)\]
<p>where \(P(y_i \mid \cdot)\) comes from a softmax over logits \(\mathbf{z}_i = \mathbf{W}_{\mathrm{ vocab}} \mathbf{t}_i \in \mathbb{R}^V\).</p>
<p>Reasoning models modify <em>what the model generates</em> and <em>how it&#39;s trained</em>, but the underlying loss machinery is the same. Let&#39;s see exactly how.</p>
<hr />
<h2 id="part_i_reasoning_via_chain_of_thought"><a href="#part_i_reasoning_via_chain_of_thought" class="header-anchor">Part I: Reasoning via Chain of Thought</a></h2>
<h3 id="the_core_idea"><a href="#the_core_idea" class="header-anchor">The Core Idea</a></h3>
<p>In <code>llm_loss.md</code>, we saw the standard autoregressive factorization for any sequence of tokens:</p>
\[P(\mathrm{ sequence}; \theta) = \prod_{i=1}^{N} P(\mathrm{ token}_i \mid \mathrm{ token}_1, \ldots, \mathrm{ token}_{i-1}; \theta)\]
<p>For reasoning tasks, we can think of the input as having two parts: a <strong>prompt</strong> or <strong>question</strong> \(x\), and the <strong>response</strong> \(y\) that the model generates. In this view, a standard LLM takes a question \(x\) and directly generates an answer \(y\):</p>
\[P(y \mid x; \theta) = \prod_{i=1}^{|y|} P(y_i \mid x, y_1, \ldots, y_{i-1}; \theta)\]
<p>This is still the same autoregressive factorization — we&#39;re just explicitly separating the &quot;given&quot; part &#40;\(x\)&#41; from the &quot;generated&quot; part &#40;\(y\)&#41;. Each factor \(P(y_i \mid x, y_1, \ldots, y_{i-1}; \theta)\) is computed via the same logit \(\rightarrow\) softmax \(\rightarrow\) cross-entropy pipeline from <code>llm_loss.md</code>, where the conditioning context includes both the original prompt tokens and the previously generated response tokens.</p>
<p>A reasoning model inserts a <strong>thinking trace</strong> \(t\) between the question and the answer:</p>
\[P(y \mid x; \theta) = \sum_t P(t \mid x; \theta) \, P(y \mid x, t; \theta)\]
<p>The trace \(t = (t_1, t_2, \ldots, t_M)\) is a sequence of tokens –- the model&#39;s &quot;scratchpad.&quot; It might look like:</p>
<pre><code class="language-julia">x: &quot;What is 27 * 34?&quot;
t: &quot;I need to multiply 27 by 34. 27 * 30 &#61; 810. 27 * 4 &#61; 108. 810 &#43; 108 &#61; 918.&quot;
y: &quot;918&quot;</code></pre>
<p>Here&#39;s the crucial point: <strong>mechanically, generating \(t\) and \(y\) uses the exact same next-token prediction</strong>. The model doesn&#39;t know that \(t\) is &quot;reasoning&quot; and \(y\) is &quot;answer.&quot; It just generates a sequence of tokens, left to right, each one predicted from all previous tokens via softmax over logits. The only difference is that we&#39;ve structured the training data &#40;or the RL reward&#41; so that generating intermediate steps before the final answer leads to better answers.</p>
<h3 id="why_does_a_longer_trace_help"><a href="#why_does_a_longer_trace_help" class="header-anchor">Why Does a Longer Trace Help?</a></h3>
<p>This is the <strong>test-time compute</strong> insight. Consider the autoregressive factorization of the full generation \((t, y)\):</p>
\[P(t, y \mid x; \theta) = \underbrace{\prod_{j=1}^{M} P(t_j \mid x, t_1, \ldots, t_{j-1}; \theta)}_{\mathrm{ generating the trace}} \cdot \underbrace{\prod_{i=1}^{|y|} P(y_i \mid x, t, y_1, \ldots, y_{i-1}; \theta)}_{\mathrm{ generating the answer}}\]
<p>When the model generates \(y_1\), it conditions on the <em>entire</em> trace \(t\). That means:</p>
<ul>
<li><p>A model with \(M = 0\) trace tokens computes \(y\) from \(x\) alone –- one &quot;shot&quot; of the transformer</p>
</li>
<li><p>A model with \(M = 200\) trace tokens has effectively run the transformer 200 additional forward steps, each time updating the key-value cache with new information</p>
</li>
</ul>
<p>Each trace token is a forward pass through the network. More trace tokens &#61; more serial computation before committing to an answer. The trace acts as <strong>external working memory</strong> that the model writes to and reads from through the attention mechanism.</p>
<p>This is why a 7B model &quot;thinking&quot; for 200 tokens can outperform a 70B model answering in 1 token: the 7B model does more total serial computation per problem.</p>
<hr />
<h2 id="part_ii_training_reasoning_models"><a href="#part_ii_training_reasoning_models" class="header-anchor">Part II: Training Reasoning Models</a></h2>
<p>There are two main approaches, and they are complementary.</p>
<h3 id="approach_1_distillation_supervised_fine-tuning"><a href="#approach_1_distillation_supervised_fine-tuning" class="header-anchor">Approach 1: Distillation &#40;Supervised Fine-Tuning&#41;</a></h3>
<p>The simplest recipe. Take a powerful model, generate \((x, t, y)\) triples, and fine-tune a smaller model on them using the standard cross-entropy loss:</p>
\[\mathcal{L}_{\mathrm{ distill}} = -\frac{1}{M + |y|} \sum_{j=1}^{M+|y|} \log P(s_j \mid s_1, \ldots, s_{j-1}; \theta)\]
<p>where \(s = (t_1, \ldots, t_M, y_1, \ldots, y_{|y|})\) is the concatenated trace-then-answer sequence.</p>
<p><strong>Key point: No architectural changes needed.</strong> This is <em>identical</em> to standard language model training. You&#39;re using the exact same transformer architecture, the exact same forward pass, the exact same backpropagation. The only difference is the training data — instead of training on raw text, you train on sequences that include reasoning traces. The loss function, the attention mechanism, the vocabulary projection: all unchanged from <code>llm_loss.md</code>.</p>
<p><strong>What does &quot;training&quot; mean here?</strong></p>
<p>You typically start with a pretrained base model &#40;e.g., Llama-3-7B, Qwen-2.5-1.5B&#41; and <strong>fine-tune</strong> it on reasoning traces. You don&#39;t train from scratch. The procedure:</p>
<ol>
<li><p>Use a teacher model to generate \((x, t, y)\) triples: prompt \(x\), reasoning trace \(t\), final answer \(y\)</p>
</li>
<li><p>Concatenate them into a single sequence: \(s = [x, t, y]\)</p>
</li>
<li><p>Fine-tune the student model on these sequences with standard cross-entropy loss</p>
</li>
<li><p>The model learns to predict reasoning tokens just like it learned to predict any other tokens during pretraining</p>
</li>
</ol>
<p><strong>But wait — doesn&#39;t this require labels/answers?</strong></p>
<p>Yes&#33; This is a key difference from standard unsupervised pretraining &#40;which only requires raw text&#41;. For reasoning model distillation, you need:</p>
<ul>
<li><p><strong>Prompts</strong> \(x\) &#40;questions like &quot;What is 27 * 34?&quot;&#41;</p>
</li>
<li><p><strong>Ground-truth answers</strong> \(y\) &#40;like &quot;918&quot;&#41;</p>
</li>
</ul>
<p>The teacher model generates the reasoning trace \(t\), but you still need to know the correct answer to verify the teacher&#39;s output is valid and to provide the final answer token for the student to learn.</p>
<p>In practice, this means you&#39;re working with <strong>supervised datasets</strong> — collections of question-answer pairs like GSM8K &#40;math problems&#41;, MATH &#40;competition math&#41;, APPS &#40;coding problems&#41;, etc. Standard pretraining doesn&#39;t need these labels; distillation does.</p>
<p><strong>Practical notes from the DeepSeek-R1 distillation experiments:</strong></p>
<ul>
<li><p>Don&#39;t use sample packing –- reasoning traces are long, and splitting them across chunks destroys coherence</p>
</li>
<li><p>Use a larger learning rate &#40;4e-5 vs. the usual 2e-5&#41; –- each doubling gained ~10 points on coding benchmarks</p>
</li>
<li><p>Prefill the prompt with a thinking token –- distilled models sometimes skip reasoning unless nudged</p>
</li>
</ul>
<h3 id="approach_2_reinforcement_learning_grpo"><a href="#approach_2_reinforcement_learning_grpo" class="header-anchor">Approach 2: Reinforcement Learning &#40;GRPO&#41;</a></h3>
<p>Distillation requires a teacher. What if you want the model to <em>discover</em> reasoning on its own?</p>
<p>This is where RL comes in. <strong>Again, no architectural changes needed.</strong> You take the same transformer that was doing standard next-token prediction, but instead of training with cross-entropy on \((x, t, y)\) triples, you let the model generate its own traces, then <strong>reward or penalize</strong> based on whether the final answer is correct.</p>
<p>The key insight: the model architecture doesn&#39;t &quot;know&quot; it&#39;s doing reasoning. From the model&#39;s perspective, it&#39;s still just predicting the next token given all previous tokens. The RL training teaches it that certain token patterns &#40;step-by-step reasoning&#41; tend to lead to higher rewards than others &#40;direct answers&#41;.</p>
<p><strong>What does &quot;training&quot; mean here?</strong></p>
<p>You typically start with a pretrained and instruction-tuned base model, then apply RL:</p>
<ol>
<li><p>Sample a prompt \(x\) from your dataset</p>
</li>
<li><p><strong>The model generates</strong> a full response \(o\) &#40;which may or may not include reasoning&#41;</p>
</li>
<li><p>Extract the final answer and check if it&#39;s correct → compute reward \(R(o, x)\)</p>
</li>
<li><p>Update the model&#39;s weights using the GRPO loss to increase probability of high-reward responses</p>
</li>
</ol>
<p><strong>Do you generate traces during training?</strong> Yes&#33; This is the key difference from distillation. During each training step, the model generates multiple candidate outputs &#40;the &quot;group&quot; of size \(G\)&#41;, and those outputs are scored by the reward function. Over time, the model learns that generating step-by-step reasoning before the final answer tends to produce correct answers more often, so it starts doing that spontaneously.</p>
<p><strong>But wait — doesn&#39;t this require labels/answers?</strong></p>
<p>Yes&#33; The reward function \(R(o, x)\) needs to verify if the model&#39;s answer is correct, which means you need ground-truth answers. This is why RL-based reasoning training focuses on <strong>verifiable domains</strong>:</p>
<ul>
<li><p><strong>Math</strong>: check if the numerical answer matches the ground truth</p>
</li>
<li><p><strong>Code</strong>: run test cases and check if they pass</p>
</li>
<li><p><strong>Formal reasoning</strong>: verify logical consistency</p>
</li>
</ul>
<p>Standard unsupervised pretraining doesn&#39;t need these labels. GRPO does. The key difference from distillation is that GRPO only needs the final answer labels — it doesn&#39;t need the reasoning traces \(t\) to be provided. The model discovers those on its own.</p>
<h4 id="from_policy_gradients_to_grpo"><a href="#from_policy_gradients_to_grpo" class="header-anchor">From Policy Gradients to GRPO</a></h4>
<p>Let&#39;s build this up carefully.</p>
<p><strong>The RL framing.</strong> We treat the language model as a <strong>policy</strong> \(\pi_\theta\) that maps a prompt \(x\) &#40;the &quot;state&quot;&#41; to a generated sequence \(o\) &#40;the &quot;action&quot;&#41;. A reward function \(R(o, x)\) scores the output &#40;e.g., \(R = 1\) if the math answer is correct, \(R = 0\) otherwise&#41;.</p>
<p>We want to maximize the expected reward:</p>
\[J(\theta) = \mathbb E_{x \sim \mathcal{D}} \left[\mathbb E_{o \sim \pi_\theta(\cdot \mid x)} [R(o, x)] \right]\]
<p>The standard policy gradient &#40;REINFORCE&#41; gives us the gradient:</p>
\[\nabla_\theta J = \mathbb E \left[ R(o, x) \nabla_\theta \log \pi_\theta(o \mid x) \right]\]
<p>This says: increase the log-probability of outputs that got high reward, decrease it for low reward. The problem is <strong>high variance</strong> –- a single sample \(o\) is a noisy estimate of what the model should do.</p>
<p><strong>PPO &#40;Proximal Policy Optimization&#41;</strong> fixes this with two tricks:</p>
<ol>
<li><p>Use a <strong>baseline</strong> &#40;value function&#41; to reduce variance: replace \(R\) with the advantage \(A = R - V(x)\)</p>
</li>
<li><p><strong>Clip</strong> the probability ratio to prevent the policy from changing too much in one step</p>
</li>
</ol>
<p>The PPO objective for a single output \(o\) is:</p>
\[\mathcal{L}_{\mathrm{ PPO}} = \min\left(\frac{\pi_\theta(o \mid x)}{\pi_{\theta_{\mathrm{ old}}}(o \mid x)} A, \; \mathrm{ clip}\left(\frac{\pi_\theta(o \mid x)}{\pi_{\theta_{\mathrm{ old}}}(o \mid x)}, 1-\epsilon, 1+\epsilon\right) A \right)\]
<p>The clipping ensures: if the advantage is positive &#40;good output&#41;, don&#39;t increase the probability ratio beyond \(1 + \epsilon\). If negative &#40;bad output&#41;, don&#39;t decrease it beyond \(1 - \epsilon\). This keeps training stable.</p>
<p><strong>The problem with PPO for LLMs:</strong> you need a value function \(V(x)\) –- a separate neural network that estimates the expected reward for each prompt. For large language models, this means training <em>another</em> model of similar size, roughly doubling memory requirements.</p>
<p><strong>GRPO &#40;Group Relative Policy Optimization&#41;</strong> eliminates the value function entirely. Instead of estimating \(V(x)\) with a neural network, GRPO:</p>
<ol>
<li><p>For each prompt \(x\), sample a <strong>group</strong> of \(G\) outputs: \(\{o_1, o_2, \ldots, o_G\} \sim \pi_{\theta_{\mathrm{ old}}}(\cdot \mid x)\)</p>
</li>
<li><p>Score each with the reward function: \(r_i = R(o_i, x)\)</p>
</li>
<li><p>Compute the advantage <strong>within the group</strong> &#40;normalize by group statistics&#41;:</p>
</li>
</ol>
\[A_i = \frac{r_i - \mathrm{ mean}(\{r_1, \ldots, r_G\})}{\mathrm{ std}(\{r_1, \ldots, r_G\})}\]
<p>This is clever: the group itself serves as the baseline. If 6 out of 8 outputs are correct &#40;\(r = 1\)&#41; and 2 are wrong &#40;\(r = 0\)&#41;, then the correct ones get a positive advantage and the wrong ones get a negative advantage. No separate value model needed.</p>
<ol start="4">
<li><p>The GRPO loss:</p>
</li>
</ol>
\[\mathcal{L}_{\mathrm{ GRPO}}(\theta) = -\mathbb E_{x} \left[\frac{1}{G}\sum_{i=1}^{G} \min\left(\frac{\pi_\theta(o_i \mid x)}{\pi_{\theta_{\mathrm{ old}}}(o_i \mid x)} A_i, \; \mathrm{ clip}(\cdot) A_i \right) - \beta \, D_{KL}(\pi_\theta \| \pi_{\mathrm{ ref}})\right]\]
<p>The KL divergence term \(D_{KL}(\pi_\theta \| \pi_{\mathrm{ ref}})\) penalizes the model for straying too far from a reference policy &#40;typically the initial supervised model&#41;. This prevents &quot;reward hacking&quot; –- the model finding degenerate outputs that exploit the reward function.</p>
<h4 id="what_the_probability_ratio_actually_means"><a href="#what_the_probability_ratio_actually_means" class="header-anchor">What the Probability Ratio Actually Means</a></h4>
<p>Let&#39;s be concrete. The ratio \(\frac{\pi_\theta(o_i \mid x)}{\pi_{\theta_{\mathrm{ old}}}(o_i \mid x)}\) is a product of per-token ratios:</p>
\[\frac{\pi_\theta(o_i \mid x)}{\pi_{\theta_{\mathrm{ old}}}(o_i \mid x)} = \prod_{k=1}^{|o_i|} \frac{P_\theta(o_{i,k} \mid x, o_{i,1}, \ldots, o_{i,k-1})}{P_{\theta_{\mathrm{ old}}}(o_{i,k} \mid x, o_{i,1}, \ldots, o_{i,k-1})}\]
<p>Each factor is a ratio of softmax probabilities –- computed via the same logits-to-probabilities pipeline from <code>llm_loss.md</code>. The only difference is that instead of comparing against a ground-truth token, we&#39;re comparing the <em>current</em> model&#39;s probability of a token against the <em>old</em> model&#39;s probability of the same token.</p>
<p>In practice, we work in log-space to avoid numerical issues:</p>
\[\log \frac{\pi_\theta(o_i \mid x)}{\pi_{\theta_{\mathrm{ old}}}(o_i \mid x)} = \sum_{k=1}^{|o_i|} \left[\log P_\theta(o_{i,k} \mid \cdot) - \log P_{\theta_{\mathrm{ old}}}(o_{i,k} \mid \cdot) \right]\]
<p>Each \(\log P_\theta(o_{i,k} \mid \cdot)\) is exactly the log-softmax of logits at position \(k\) –- the same quantity that appears in the cross-entropy loss, just evaluated at the <em>sampled</em> token instead of a <em>ground-truth</em> token.</p>
<h4 id="the_remarkable_finding_emergent_reasoning_r1-zero"><a href="#the_remarkable_finding_emergent_reasoning_r1-zero" class="header-anchor">The Remarkable Finding: Emergent Reasoning &#40;R1-Zero&#41;</a></h4>
<p>When you apply GRPO directly to a base model &#40;no supervised fine-tuning, no reasoning traces in the training data&#41;, the model spontaneously develops reasoning behaviors:</p>
<ul>
<li><p><strong>Self-verification</strong>: &quot;Let me check this answer...&quot;</p>
</li>
<li><p><strong>Backtracking</strong>: &quot;Wait, that&#39;s wrong. Let me try again...&quot;</p>
</li>
<li><p><strong>Strategy exploration</strong>: trying multiple approaches before committing</p>
</li>
</ul>
<p>These behaviors <strong>emerge</strong> because the RL objective rewards correct final answers. The model discovers on its own that &quot;thinking before answering&quot; is a strategy that increases reward. Nobody programmed these patterns –- the optimization landscape favors them.</p>
<h4 id="distillation_vs_rl_the_trade-off"><a href="#distillation_vs_rl_the_trade-off" class="header-anchor">Distillation vs. RL: The Trade-off</a></h4>
<table><tr><th align="left">Property</th><th align="left">Distillation</th><th align="left">GRPO</th></tr><tr><td align="left">Training loss</td><td align="left">Cross-entropy &#40;standard LM loss&#41;</td><td align="left">Clipped policy gradient &#43; KL penalty</td></tr><tr><td align="left">Requires</td><td align="left">Teacher model traces</td><td align="left">Reward function &#40;verifier&#41;</td></tr><tr><td align="left">What model learns</td><td align="left">To imitate teacher&#39;s reasoning</td><td align="left">Its own reasoning strategies</td></tr><tr><td align="left">Data efficiency</td><td align="left">Needs many \((x, t, y)\) examples</td><td align="left">Needs prompts \(x\) &#43; reward signal</td></tr><tr><td align="left">Compute cost</td><td align="left">Standard fine-tuning</td><td align="left">Higher &#40;generate \(G\) samples per step&#41;</td></tr><tr><td align="left">Surprise factor</td><td align="left">Predictable &#40;copies teacher&#41;</td><td align="left">Can discover novel strategies</td></tr></table>
<p>The DeepSeek-R1 pipeline uses both: RL first to develop reasoning ability, then distillation to transfer that ability to smaller models.</p>
<hr />
<h2 id="part_iii_a_different_kind_of_reasoning_-_recursive_refinement"><a href="#part_iii_a_different_kind_of_reasoning_-_recursive_refinement" class="header-anchor">Part III: A Different Kind of Reasoning –- Recursive Refinement</a></h2>
<p>Everything above is about <strong>autoregressive</strong> reasoning: the model generates tokens left-to-right, and the trace is expressed in natural language. But there&#39;s a completely different approach, explored in the <a href="/blog/machine-learning/tiny_recursive_models/">TRM paper</a> &#40;Jolicoeur-Martineau, 2025&#41;.</p>
<h3 id="the_problem_with_autoregressive_reasoning"><a href="#the_problem_with_autoregressive_reasoning" class="header-anchor">The Problem with Autoregressive Reasoning</a></h3>
<p>Chain-of-thought reasoning has a fundamental fragility: it&#39;s <strong>sequential and irreversible</strong>. Each token is generated conditioned on all previous tokens. If token \(t_{j}\) is wrong, every subsequent token \(t_{j+1}, t_{j+2}, \ldots\) is conditioned on that error. The model can try to &quot;backtrack&quot; in text &#40;&quot;Wait, that&#39;s wrong...&quot;&#41;, but it&#39;s generating more tokens to fix earlier tokens –- it can&#39;t actually rewrite the past.</p>
<p>Also, reasoning in natural language is <em>expensive</em>. Each reasoning token requires a full forward pass through the model. A 7B model generating 2000 reasoning tokens does \(2000 \times 7\mathrm{ B} = 14\) trillion multiply-adds just for thinking.</p>
<h3 id="recursive_refinement_think_in_vectors_not_words"><a href="#recursive_refinement_think_in_vectors_not_words" class="header-anchor">Recursive Refinement: Think in Vectors, Not Words</a></h3>
<p>TRM &#40;Tiny Recursive Model&#41; takes a radically different approach. <strong>This requires major architectural changes.</strong> Instead of generating a text trace autoregressively, it maintains <strong>latent state vectors</strong> and iteratively refines them by running the same tiny network over and over.</p>
<p>The setup abandons language modeling entirely in favor of supervised learning. Given an input \(x\) &#40;a puzzle&#41; and target \(y^*\) &#40;the solution&#41;, both tokenized into sequences:</p>
\[x \in \{1, \ldots, V\}^{L_x}, \quad y^* \in \{1, \ldots, V\}^{L_y}\]
<p><strong>Why the same length \(L\) in practice?</strong> For simplicity, TRM assumes \(x\) and \(y\) have the same sequence length. If the natural input and output have different lengths, you pad the shorter one. For example:</p>
<ul>
<li><p><strong>Sudoku</strong>: Input is a 9×9 grid with blanks &#40;81 tokens&#41;, output is the completed grid &#40;81 tokens&#41; — naturally the same length</p>
</li>
<li><p><strong>Mazes</strong>: Input is the maze layout, output is the solution path — pad whichever is shorter to match</p>
</li>
<li><p><strong>ARC-AGI</strong>: Input and output grids may differ in size — pad to a common maximum length</p>
</li>
</ul>
<p>This is a <strong>supervised learning</strong> setup, not a generative language modeling task. You&#39;re training the network to map fixed-size inputs to fixed-size outputs, like image-to-image translation. The padding is just a practical detail to make the matrix operations work.</p>
<p><strong>Architectural differences from standard transformers:</strong></p>
<ol>
<li><p><strong>No autoregressive generation</strong>: The model doesn&#39;t generate tokens left-to-right</p>
</li>
<li><p><strong>Stateful operation</strong>: The model maintains evolving state vectors \((y, z)\) across multiple forward passes</p>
</li>
<li><p><strong>Weight sharing</strong>: The same 2-layer transformer is applied recursively, rather than having separate layers</p>
</li>
<li><p><strong>Input conditioning</strong>: The network receives different inputs at different steps — sometimes \((x, y, z)\), sometimes just \((y, z)\)</p>
</li>
</ol>
<p>The model maintains three variables as continuous embeddings:</p>
<ul>
<li><p>\(x \in \mathbb{R}^{L \times D}\) –- the input &#40;fixed, embedded from discrete tokens&#41;</p>
</li>
<li><p>\(y \in \mathbb{R}^{L \times D}\) –- the current answer estimate &#40;evolves, lives in embedding space&#41;</p>
</li>
<li><p>\(z \in \mathbb{R}^{L \times D}\) –- the latent reasoning state &#40;evolves, lives in a <strong>different learned space</strong>&#41;</p>
</li>
</ul>
<p><strong>Critical point about \(z\):</strong> The reasoning trace \(z\) is <strong>not</strong> in the same space as the token embeddings. Here&#39;s the distinction:</p>
<ul>
<li><p>\(x\) and \(y\): These start as discrete token IDs, get embedded into \(\mathbb{R}^D\), and can be decoded back to tokens via \(\mathbf{W}_{\mathrm{ vocab}}\)</p>
</li>
<li><p>\(z\): This is a <strong>pure latent vector</strong> that the network learns to use as working memory. It&#39;s not tied to any vocabulary. If you project \(z\) through \(\mathbf{W}_{\mathrm{ vocab}}\) and decode it, you get gibberish — it was never meant to be interpretable as tokens</p>
</li>
</ul>
<p>Think of it this way:</p>
<ul>
<li><p>In CoT models, reasoning happens in <strong>token space</strong> &#40;natural language you can read&#41;</p>
</li>
<li><p>In TRM, reasoning happens in <strong>latent space</strong> &#40;learned representations with no direct token correspondence&#41;</p>
</li>
</ul>
<p>This is why TRM can&#39;t explain its reasoning in human language — the trace \(z\) is optimized purely for computational utility, not linguistic interpretability.</p>
<p><strong>What does &quot;training&quot; mean here?</strong></p>
<p>You build and train this custom network <strong>from scratch</strong> &#40;no pretrained base model&#41;. For each training example:</p>
<ol>
<li><p>Start with random initializations for \(y\) and \(z\)</p>
</li>
<li><p>Run the recursive refinement loop &#40;latent updates, answer updates, repeated across multiple supervision steps&#41;</p>
</li>
<li><p>After each supervision step, compute cross-entropy between the predicted answer and ground truth</p>
</li>
<li><p>Backpropagate the loss and update the 2-layer network&#39;s weights</p>
</li>
</ol>
<p><strong>Do you generate traces during training?</strong> Not in natural language. The &quot;reasoning&quot; happens entirely in the latent vector \(z\), which is never decoded to text. The model refines \(z\) over many forward passes, and eventually uses it to produce the final answer \(y\). But if you tried to decode \(z\) to tokens at any intermediate step, you&#39;d get nonsense — it&#39;s internal working memory, not a linguistic trace.</p>
<p>This is fundamentally different from CoT models, where the reasoning is explicitly generated as text tokens that you can read.</p>
<p><strong>But wait — doesn&#39;t this require labels/answers?</strong></p>
<p>Absolutely&#33; TRM is <strong>fully supervised</strong>. For every training example, you need:</p>
<ul>
<li><p>Input puzzle \(x\) &#40;e.g., a Sudoku grid with blanks&#41;</p>
</li>
<li><p>Ground-truth solution \(y^*\) &#40;the completed grid&#41;</p>
</li>
</ul>
<p>This is pure supervised learning, like training an image classifier. You optimize the model to produce outputs that match the labels. No unsupervised pretraining phase — just direct optimization on \((x, y^*)\) pairs.</p>
<p>The TRM paper uses ~1000 labeled examples per task &#40;Sudoku, mazes, ARC-AGI&#41;, with heavy data augmentation to prevent overfitting.</p>
<p>A single 2-layer transformer \(f_\theta\) is applied recursively:</p>
<p><strong>Latent update</strong> &#40;\(n\) times&#41;: refine the reasoning state given the full context</p>
\[z \leftarrow f_\theta([x; y; z])\]
<p><strong>Answer update</strong> &#40;once&#41;: update the answer from the reasoning state</p>
\[y \leftarrow f_\theta([y; z])\]
<p>where \([a; b; c]\) denotes concatenation along the sequence dimension.</p>
<p>Notice the key difference: when updating \(z\), the network sees the input \(x\). When updating \(y\), it does not. This input distinction lets one network play two roles –- &quot;reasoner&quot; &#40;sees the question&#41; and &quot;summarizer&quot; &#40;produces the answer&#41;.</p>
<p><strong>Why not just one forward pass through a transformer?</strong></p>
<p>You might ask: &quot;A transformer already has self-attention that lets position 2 look at position 40. Why not just do one forward pass \(x \rightarrow y\) and be done?&quot;</p>
<p>Great question&#33; The issue is <strong>depth of reasoning</strong> vs <strong>width of information gathering</strong>.</p>
<p>One forward pass through a transformer gives you:</p>
<ul>
<li><p>✓ <strong>Information gathering</strong>: Position 2 can attend to all other positions &#40;rows, columns, blocks in Sudoku&#41;</p>
</li>
<li><p>✗ <strong>Iterative refinement</strong>: You only get one &quot;shot&quot; at solving the puzzle</p>
</li>
</ul>
<p>But constraint satisfaction problems require <strong>multiple rounds of inference</strong>:</p>
<pre><code class="language-julia">Round 1: &quot;Position 5 can&#39;t be 3 or 7 &#40;those are in the same row&#41;&quot;
Round 2: &quot;Now that I know position 5 isn&#39;t 3, position 8 must be 3&quot;
Round 3: &quot;Since position 8 is 3, position 12 can&#39;t be 3...&quot;</code></pre>
<p>This is <strong>serial reasoning</strong> — each conclusion enables the next one. A single forward pass, even with perfect attention, can&#39;t do this because all positions are computed in parallel from the input.</p>
<p><strong>TRM&#39;s recursion</strong> solves this by running the same small transformer <strong>many times</strong>:</p>
\[\begin{aligned}
\mathrm{ Pass 1:} & \quad z^{(1)} = f_\theta([x; y^{(0)}; z^{(0)}]) \quad \mathrm{ (initial constraints)} \\
\mathrm{ Pass 2:} & \quad z^{(2)} = f_\theta([x; y^{(0)}; z^{(1)}]) \quad \mathrm{ (propagate conclusions)} \\
& \quad \vdots \\
\mathrm{ Pass 672:} & \quad z^{(672)} = f_\theta([x; y^{(*)}, z^{(*)}]) \quad \mathrm{ (final answer)}
\end{aligned}\]
<p>Each pass updates \(z\) based on the conclusions from the previous pass. After 672 applications of the same 2-layer network, you&#39;ve done 672 &quot;rounds&quot; of constraint propagation.</p>
<p><strong>Why not just make the transformer deeper?</strong> You could build a 672-layer transformer and do one pass. But:</p>
<ol>
<li><p><strong>Parameter explosion</strong>: 672 separate layers &#61; hundreds of millions of parameters → overfits on 1000 examples</p>
</li>
<li><p><strong>No weight sharing</strong>: Each layer learns different computations, so you need massive data</p>
</li>
<li><p><strong>No recurrence</strong>: Can&#39;t naturally model &quot;repeat this reasoning step until converged&quot;</p>
</li>
</ol>
<p>TRM&#39;s recursion gives you the <strong>depth</strong> &#40;672 effective layers&#41; with the <strong>parameter efficiency</strong> &#40;5M parameters, reused 672 times&#41;. It&#39;s depth through iteration, not through stacking layers.</p>
<p><strong>The analogy</strong>: Think of solving a Sudoku:</p>
<ul>
<li><p><strong>One forward pass</strong>: Looking at the entire grid once and writing down all answers simultaneously</p>
</li>
<li><p><strong>TRM recursion</strong>: Looking at the grid, making one deduction, looking again with that new information, making another deduction, repeat 672 times</p>
</li>
</ul>
<p>Humans solve Sudoku the second way. TRM does too.</p>
<p><strong>What does the latent state \(z\) actually learn?</strong> The intermediate &quot;reasoning&quot; stored in \(z\) across the 672 passes encodes <strong>partial solutions and constraints</strong> that make the final answer easier to arrive at. For example, in Sudoku:</p>
<ul>
<li><p>Early passes: \(z\) might encode &quot;position 5 can be 2, 4, or 6&quot; &#40;eliminating impossible values&#41;</p>
</li>
<li><p>Middle passes: \(z\) might encode &quot;if position 5 is 2, then position 8 must be 7&quot; &#40;conditional constraints&#41;</p>
</li>
<li><p>Late passes: \(z\) converges to &quot;position 5 is definitely 4&quot; &#40;commitment&#41;</p>
</li>
</ul>
<p>The final answer \(y\) is computed from this refined \(z\) state. The loss at each supervision step encourages \(z\) to progressively refine toward states that make correct predictions easier.</p>
<p>This is exactly analogous to CoT reasoning in language models — the trace \(t\) &#40;&quot;27 × 30 &#61; 810, 27 × 4 &#61; 108, 810 &#43; 108 &#61; ...&quot;&#41; stores intermediate results that make the final answer &#40;&quot;918&quot;&#41; easier to produce. The difference is:</p>
<ul>
<li><p><strong>CoT</strong>: Intermediate steps are explicit tokens you can read</p>
</li>
<li><p><strong>TRM</strong>: Intermediate steps are latent vectors optimized purely for computational utility</p>
</li>
</ul>
<h3 id="the_mathematics_of_trm"><a href="#the_mathematics_of_trm" class="header-anchor">The Mathematics of TRM</a></h3>
<p>Let&#39;s be completely explicit about what happens. Given a training example with discrete tokens:</p>
\[x_{\mathrm{ discrete}} \in \{1, \ldots, V\}^L, \quad y^*_{\mathrm{ discrete}} \in \{1, \ldots, V\}^L\]
<p><strong>Step 1: Embedding.</strong> Convert discrete tokens to continuous vectors via an embedding matrix \(\mathbf{E} \in \mathbb{R}^{V \times D}\):</p>
\[x = \mathbf{E}[x_{\mathrm{ discrete}}] \in \mathbb{R}^{L \times D}\]
<p>where \(\mathbf{E}[k]\) retrieves the \(k\)-th row of \(\mathbf{E}\) &#40;the embedding for token \(k\)&#41;.</p>
<p><strong>Step 2: Initialize states.</strong> Start with random or zero initialization:</p>
\[y^{(0)} = \mathbf{0} \in \mathbb{R}^{L \times D}, \quad z^{(0)} = \mathbf{0} \in \mathbb{R}^{L \times D}\]
<p><strong>Step 3: Recursive refinement.</strong> This is where the notation gets dense. Let me unpack it carefully.</p>
<p>We have three levels of nesting:</p>
<ul>
<li><p><strong>Supervision steps</strong> \(s = 1, \ldots, N_{\mathrm{ sup}}\): how many times we snapshot the state and compute a loss</p>
</li>
<li><p><strong>Outer repetitions</strong> \(t = 1, \ldots, T\): how many times we repeat the latent-update loop</p>
</li>
<li><p><strong>Inner latent updates</strong> \(i = 1, \ldots, n\): how many times we refine \(z\) before updating \(y\)</p>
</li>
</ul>
<p>The superscripts \((s, t, i)\) track where we are in this nested loop. Here&#39;s the procedure:</p>
<p><strong>Initialize for supervision step \(s\):</strong></p>
<ul>
<li><p>If \(s = 1\): start with \(y^{(1,0)} = y^{(0)} = \mathbf{0}\) and \(z^{(1,0)} = z^{(0)} = \mathbf{0}\)</p>
</li>
<li><p>If \(s > 1\): carry over from the previous step: \(y^{(s,0)} = y^{(s-1,T)}\) and \(z^{(s,0)} = z^{(s-1,T)}\)</p>
</li>
</ul>
<p><strong>For each repetition \(t = 1, \ldots, T\):</strong></p>
<p>Start with the states from the previous repetition: \(y^{(s,t,0)} = y^{(s,t-1)}\) and \(z^{(s,t,0)} = z^{(s,t-1)}\)</p>
<p>Then do \(n\) latent updates:</p>
\[\mathrm{ For} i = 1, \ldots, n: \quad z^{(s,t,i)} = f_\theta\left(\mathrm{ concat}(x, y^{(s,t,i-1)}, z^{(s,t,i-1)})\right)\]
<p>Notice: \(y\) doesn&#39;t change during these \(n\) updates. We&#39;re just refining \(z\), so \(y^{(s,t,i)} = y^{(s,t,0)}\) for \(i = 1, \ldots, n\).</p>
<p>After the \(n\)-th latent update, we have \(z^{(s,t,n)}\). Now update \(y\)<strong>once</strong>:</p>
\[y^{(s,t)} = f_\theta\left(\mathrm{ concat}(y^{(s,t,n)}, z^{(s,t,n)})\right)\]
<p>Here \(y^{(s,t,n)}\) is just the carried-over \(y\) value &#40;which equals \(y^{(s,t,0)}\)&#41;, and we&#39;re using the refined latent state \(z^{(s,t,n)}\) to compute the new \(y^{(s,t)}\).</p>
<p><strong>Summary of what \(y^{(\cdot)}\) means:</strong></p>
<ul>
<li><p>\(y^{(s,t)}\): the answer state after repetition \(t\) within supervision step \(s\)</p>
</li>
<li><p>\(y^{(s,t,i)}\): the answer state during the \(i\)-th latent update of repetition \(t\) &#40;doesn&#39;t change, equals \(y^{(s,t,0)}\)&#41;</p>
</li>
</ul>
<p>The \(\mathrm{ concat}\) operation stacks matrices along the sequence dimension: if \(a \in \mathbb{R}^{L \times D}\), \(b \in \mathbb{R}^{L \times D}\), then \(\mathrm{ concat}(a, b) \in \mathbb{R}^{(2L) \times D}\).</p>
<p><strong>Step 4: Decode to logits.</strong> Project the final answer embedding back to vocabulary space:</p>
\[\hat{y}^{(s)} = y^{(s,T)} \mathbf{W}_{\mathrm{ vocab}}^\mathsf{T} \in \mathbb{R}^{L \times V}\]
<p>where \(\mathbf{W}_{\mathrm{ vocab}} \in \mathbb{R}^{V \times D}\) is the vocabulary projection matrix &#40;same as in standard transformers&#41;.</p>
<p><strong>Step 5: Compute loss.</strong> For each position \(\ell = 1, \ldots, L\) and each supervision step \(s\):</p>
\[\mathcal{L}_s = -\frac{1}{L}\sum_{\ell=1}^{L} \log \frac{\exp(\hat{y}^{(s)}[\ell, y^*_{\mathrm{ discrete}}[\ell]])}{\sum_{v=1}^{V} \exp(\hat{y}^{(s)}[\ell, v])}\]
<p>This is cross-entropy between the predicted token distribution at position \(\ell\) and the ground-truth token \(y^*_{\mathrm{ discrete}}[\ell]\).</p>
<p><strong>Step 6: Total loss.</strong> Sum over all supervision steps:</p>
\[\mathcal{L}_{\mathrm{ total}} = \sum_{s=1}^{N_{\mathrm{ sup}}} \mathcal{L}_s\]
<p>with gradients detached between steps &#40;backprop only through the last \(T\) repetition within each step&#41;.</p>
<p><strong>Key observations:</strong></p>
<ol>
<li><p><strong>\(z\) never touches \(\mathbf{W}_{\mathrm{ vocab}}\)</strong>: Only \(y\) gets projected to logits. The latent reasoning state \(z\) is never decoded to tokens.</p>
</li>
<li><p><strong>Same network, different inputs</strong>: \(f_\theta\) is applied with different concatenations — sometimes \([x; y; z]\), sometimes \([y; z]\) — but it&#39;s the same 2-layer transformer weights.</p>
</li>
<li><p><strong>Effective depth from repetition</strong>: Even though \(f_\theta\) has only 2 layers, applying it \((n+1) \times T \times N_{\mathrm{ sup}}\) times creates enormous effective depth through recurrence.</p>
</li>
</ol>
<h3 id="the_recursion_structure_summary"><a href="#the_recursion_structure_summary" class="header-anchor">The Recursion Structure &#40;Summary&#41;</a></h3>
<p>One &quot;supervision step&quot; works as follows. Define the inner recursion:</p>
\[\mathrm{ latent\_recursion}(x, y, z) : \quad \begin{cases} z \leftarrow f_\theta([x; y; z]) & \mathrm{ repeated} n \mathrm{ times} \\ y \leftarrow f_\theta([y; z]) & \mathrm{ once} \end{cases}\]
<p>Then the outer recursion repeats this \(T\) times, but only backpropagates through the last repetition:</p>
\[\mathrm{ deep\_recursion}(x, y, z): \quad \begin{cases} (y, z) \leftarrow \mathrm{ latent\_recursion}(x, y, z) & T - 1 \mathrm{ times, no gradients} \\ (y, z) \leftarrow \mathrm{ latent\_recursion}(x, y, z) & \mathrm{ once, with gradients} \end{cases}\]
<p>Finally, <strong>deep supervision</strong> runs \(N_{\mathrm{ sup}}\) supervision steps in sequence, computing a loss after each one:</p>
\[\hat{y} = \mathbf{W}_{\mathrm{ vocab}} \, y, \quad \mathcal{L} = \mathrm{ CrossEntropy}(\hat{y}, \; y^*)\]
<p>The latent state \((y, z)\) carries over between supervision steps, but gradients are detached. This means each step trains independently while benefiting from the accumulated reasoning of previous steps.</p>
<h3 id="effective_depth"><a href="#effective_depth" class="header-anchor">Effective Depth</a></h3>
<p>With \(n = 6\) latent updates, \(T = 3\) repetitions, 2 layers, and \(N_{\mathrm{ sup}} = 16\) supervision steps:</p>
<ul>
<li><p><strong>Per supervision step</strong>: \(T \times (n + 1) \times 2 = 3 \times 7 \times 2 = 42\) effective layers</p>
</li>
<li><p><strong>Total</strong>: \(42 \times 16 = 672\) effective layers</p>
</li>
</ul>
<p>All from a 2-layer, 7-million-parameter network. The same weights are reused for every single one of those 672 effective layers.</p>
<h3 id="why_tiny_networks_win_here"><a href="#why_tiny_networks_win_here" class="header-anchor">Why Tiny Networks Win Here</a></h3>
<p>The ablation tells the story:</p>
<table><tr><th align="left">Change from TRM</th><th align="center">Test Accuracy</th><th align="center">Parameters</th></tr><tr><td align="left">TRM &#40;2 layers, single network, full backprop&#41;</td><td align="center"><strong>87.4&#37;</strong></td><td align="center"><strong>5M</strong></td></tr><tr><td align="left">&#43; separate reasoning and answer networks</td><td align="center">82.4&#37;</td><td align="center">10M</td></tr><tr><td align="left">&#43; 4 layers per network</td><td align="center">79.5&#37;</td><td align="center">10M</td></tr><tr><td align="left">&#43; self-attention &#40;instead of MLP&#41;</td><td align="center">74.7&#37;</td><td align="center">7M</td></tr><tr><td align="left">&#43; 1-step gradient approximation</td><td align="center">56.5&#37;</td><td align="center">5M</td></tr></table>
<p>Every increase in model capacity <strong>hurts</strong>. Why? With only ~1000 training examples, larger models overfit. Recursion provides depth &#40;and therefore expressiveness&#41; without adding parameters. It&#39;s the ideal inductive bias for the low-data regime.</p>
<h3 id="trm_loss_vs_llm_loss"><a href="#trm_loss_vs_llm_loss" class="header-anchor">TRM Loss vs. LLM Loss</a></h3>
<p>Let&#39;s make the connection explicit. Recall from <code>llm_loss.md</code>:</p>
<p><strong>Standard LLM &#40;next-token prediction&#41;:</strong></p>
\[\mathcal{L}_{\mathrm{ LLM}} = -\frac{1}{N}\sum_{i=1}^{N} \log P(y_i \mid y_1, \ldots, y_{i-1}; \theta)\]
<p>One forward pass, one loss computation, trained on billions of tokens.</p>
<p><strong>Reasoning LLM &#40;GRPO&#41;:</strong></p>
<p>Same next-token prediction for generating traces, but trained with:</p>
\[\mathcal{L}_{\mathrm{ GRPO}} = -\frac{1}{G}\sum_{i=1}^{G} \mathrm{ clip}\left(\frac{\pi_\theta(o_i \mid x)}{\pi_{\theta_{\mathrm{ old}}}(o_i \mid x)}\right) A_i + \beta \, D_{KL}(\pi_\theta \| \pi_{\mathrm{ ref}})\]
<p>Still autoregressive, still predicting tokens, but optimized for downstream reward rather than teacher imitation.</p>
<p><strong>TRM &#40;recursive refinement&#41;:</strong></p>
\[\mathcal{L}_{\mathrm{ TRM}} = \sum_{s=1}^{N_{\mathrm{ sup}}} \mathrm{ CrossEntropy}\left(\mathbf{W}_{\mathrm{ vocab}} \cdot y^{(s)}, \; y^*\right)\]
<p>where \(y^{(s)}\) is the answer state after \(s\) supervision steps, each involving \(T \times (n + 1)\) forward passes through the same 2-layer network. Not autoregressive. No trace in natural language. Just iterative vector refinement.</p>
<hr />
<h2 id="part_iv_the_two_philosophies_side_by_side"><a href="#part_iv_the_two_philosophies_side_by_side" class="header-anchor">Part IV: The Two Philosophies Side by Side</a></h2>
<table><tr><th align="left"></th><th align="left">Autoregressive Reasoning &#40;CoT&#41;</th><th align="left">Recursive Refinement &#40;TRM&#41;</th></tr><tr><td align="left"><strong>Architecture changes?</strong></td><td align="left"><strong>No</strong> — same transformer</td><td align="left"><strong>Yes</strong> — custom recursive network</td></tr><tr><td align="left"><strong>Training changes?</strong></td><td align="left"><strong>Yes</strong> — different data or RL</td><td align="left"><strong>Yes</strong> — supervised &#43; deep supervision</td></tr><tr><td align="left"><strong>Start from pretrained model?</strong></td><td align="left"><strong>Yes</strong> — fine-tune a base LLM</td><td align="left"><strong>No</strong> — train from scratch</td></tr><tr><td align="left"><strong>Requires ground-truth labels?</strong></td><td align="left"><strong>Yes</strong> — need correct answers for both distillation and RL</td><td align="left"><strong>Yes</strong> — fully supervised \((x, y^*)\) pairs</td></tr><tr><td align="left"><strong>Generate reasoning during training?</strong></td><td align="left">Distillation: No &#40;provided&#41;. RL: Yes &#40;sampled&#41;.</td><td align="left">No — reasoning stays in latent vectors</td></tr><tr><td align="left"><strong>Reasoning medium</strong></td><td align="left">Natural language tokens</td><td align="left">Learned latent vectors</td></tr><tr><td align="left"><strong>Reasoning is</strong></td><td align="left">Explicit, human-readable</td><td align="left">Implicit, not decodable</td></tr><tr><td align="left"><strong>Architecture</strong></td><td align="left">Large transformer &#40;7B–671B&#41;</td><td align="left">Tiny transformer &#40;2 layers, 7M&#41;</td></tr><tr><td align="left"><strong>Training paradigm</strong></td><td align="left">Distillation &#40;supervised&#41; or RL &#40;reward-based&#41;</td><td align="left">Fully supervised</td></tr><tr><td align="left"><strong>Data needed</strong></td><td align="left">Billions of tokens &#40;pretraining&#41; &#43; labeled Q&amp;A</td><td align="left">~1000 task-specific labeled examples</td></tr><tr><td align="left"><strong>Depth</strong></td><td align="left">Sequential token generation</td><td align="left">Recursive weight-shared forward passes</td></tr><tr><td align="left"><strong>Generality</strong></td><td align="left">General-purpose</td><td align="left">Task-specific</td></tr><tr><td align="left"><strong>Error recovery</strong></td><td align="left">Textual backtracking &#40;fragile&#41;</td><td align="left">Iterative refinement &#40;robust&#41;</td></tr><tr><td align="left"><strong>Where it wins</strong></td><td align="left">Language, broad knowledge</td><td align="left">Structured puzzles, constraint satisfaction</td></tr></table>
<h3 id="the_common_thread"><a href="#the_common_thread" class="header-anchor">The Common Thread</a></h3>
<p>Despite their differences, both approaches share the same core insight: <strong>reasoning requires depth, and depth can come from iteration rather than parameters.</strong></p>
<p>For autoregressive models, &quot;iteration&quot; means generating more tokens &#40;test-time compute&#41;. For TRM, it means running the same network more times &#40;recursive compute&#41;. Both trade FLOPs for effective depth. Both allow smaller models to solve problems that larger, shallower models cannot.</p>
<p>The question is which kind of iteration suits your problem:</p>
<ul>
<li><p>If you need <strong>general-purpose</strong> reasoning with <strong>natural-language</strong> explanations: autoregressive CoT</p>
</li>
<li><p>If you need <strong>task-specific</strong> reasoning on <strong>structured problems</strong> with <strong>limited data</strong>: recursive refinement</p>
</li>
</ul>
<hr />
<h2 id="practical_summary"><a href="#practical_summary" class="header-anchor">Practical Summary</a></h2>
<p><strong>If you want to use reasoning models &#40;inference&#41;:</strong></p>
<p>The loss function during inference is irrelevant –- you just sample tokens. But understanding the training loss tells you <em>why</em> the model reasons:</p>
<ul>
<li><p>Distilled models reason because they were trained on reasoning traces &#40;cross-entropy&#41;</p>
</li>
<li><p>RL-trained models reason because reasoning increases reward &#40;GRPO&#41;</p>
</li>
<li><p>TRM reasons because iterative refinement reduces supervised loss &#40;deep supervision&#41;</p>
</li>
</ul>
<p><strong>If you want to train reasoning models:</strong></p>
<table><tr><th align="left">Goal</th><th align="left">Method</th><th align="left">Compute needed</th></tr><tr><td align="left">Add reasoning to a small LLM</td><td align="left">Distillation from a teacher</td><td align="left">Standard fine-tuning &#40;a few GPUs&#41;</td></tr><tr><td align="left">Train reasoning from scratch &#40;LLM&#41;</td><td align="left">GRPO with verifiable rewards</td><td align="left">High &#40;sample \(G\) outputs per step&#41;</td></tr><tr><td align="left">Solve structured puzzles</td><td align="left">TRM-style recursive refinement</td><td align="left">Low &#40;single GPU, hours&#41;</td></tr></table>
<p><strong>If you&#39;re in academia with limited compute:</strong></p>
<p>TRM is the most accessible. A 5M-parameter model trained in 18 hours on a single L40S that beats 671B-parameter models on puzzle benchmarks. The <a href="/blog/machine-learning/tiny_recursive_models/">TRM blog post</a> covers the full details, including code to reproduce the results.</p>
<hr />
<h2 id="key_references"><a href="#key_references" class="header-anchor">Key References</a></h2>
<ul>
<li><p><strong>DeepSeek-R1</strong>: <a href="https://arxiv.org/abs/2501.12948">arXiv:2501.12948</a> –- GRPO, emergent reasoning, distillation pipeline</p>
</li>
<li><p><strong>TRM</strong>: <a href="https://arxiv.org/abs/2510.04871">arXiv:2510.04871</a> –- recursive refinement with tiny networks</p>
</li>
<li><p><strong>The Transformer Loss Function</strong>: <a href="/blog/machine-learning/llm_loss/">llm_loss</a> –- the foundation this post builds on</p>
</li>
<li><p><strong>Open-R1</strong>: <a href="https://github.com/huggingface/open-r1">github.com/huggingface/open-r1</a> –- open-source reproduction of R1</p>
</li>
<li><p><strong>TRL</strong>: <a href="https://github.com/huggingface/trl">github.com/huggingface/trl</a> –- GRPO implementation</p>
</li>
</ul>
<div class="page-foot">
    Contact me by <a href="mailto:skchu@wustl.edu">E-mail</a> | <a href="https://github.com/kchu25">Github</a> | <a href="https://www.linkedin.com/in/kchu1/">Linkedin</a>
    <br>
    This work is licensed under <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a>.  Last modified: February 07, 2026.
    <br>
    Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia language</a>.
</div>
</div><!-- CONTENT ENDS HERE -->
    
        <script src="/libs/katex/katex.min.js"></script>
<script src="/libs/katex/contrib/auto-render.min.js"></script>
<script>renderMathInElement(document.body)</script>

    
    
        <script src="/libs/highlight/highlight.min.js"></script>
<script>hljs.highlightAll();hljs.configure({tabReplace: '    '});</script>

    
  </body>
</html>
