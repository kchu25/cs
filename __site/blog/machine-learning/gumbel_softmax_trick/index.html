<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
   <link rel="stylesheet" href="/libs/katex/katex.min.css">
     
   <link rel="stylesheet" href="/libs/highlight/styles/github.min.css">
   
  <link rel="stylesheet" href="/css/franklin.css">
<link rel="stylesheet" href="/css/hypertext.css">
<link rel="icon" href="/assets/favicon.png">

   <title>The Gumbel-Softmax Trick </title>  
</head>
<body>
<header>
  <h1 style="color:#283747">computer science</h1>
  <nav>
    <a href="/" class="current">Tags</a>
  | <a href="/blog/" >Notes</a>
  <hr/>
  </nav>
</header>


<!-- Content appended here -->
<div class="franklin-content"><p><strong>Bottom line:</strong> Static p is simpler and more common &#40;especially for pruning&#41;. Adaptive p is more powerful but more complex. Choose based on whether your gating decision should be <em>structural</em> &#40;static&#41; or <em>contextual</em> &#40;adaptive&#41;&#33;</p>
<h3 id="the_sigmoid_saturation_problem"><a href="#the_sigmoid_saturation_problem" class="header-anchor">The Sigmoid Saturation Problem</a></h3>
<p><strong>You&#39;re absolutely right to worry about this&#33;</strong> Let&#39;s talk about the parameterization of p.</p>
<h4 id="the_naive_approach_has_problems"><a href="#the_naive_approach_has_problems" class="header-anchor">The Naive Approach &#40;Has Problems&#33;&#41;</a></h4>
<pre><code class="language-julia">struct NaiveBernoulliGate
    logit::Float32  # Unconstrained parameter
end

function get_probability&#40;gate::NaiveBernoulliGate&#41;
    return œÉ&#40;gate.logit&#41;  # p &#61; sigmoid&#40;logit&#41;
end</code></pre>
<p><strong>The saturation problem:</strong></p>
<p>Sigmoid saturates at the extremes:</p>
<ul>
<li><p>When <code>logit &#61; 10</code>: <code>p ‚âà 0.99999</code> &#40;almost 1, gradient ‚âà 0&#41;</p>
</li>
<li><p>When <code>logit &#61; -10</code>: <code>p ‚âà 0.00001</code> &#40;almost 0, gradient ‚âà 0&#41;</p>
</li>
</ul>
<pre><code class="language-julia">p &#61; œÉ&#40;logit&#41;
    1.0 |         ___________  ‚Üê saturated&#33; &#40;gradient ‚âà 0&#41;
        |       /
    0.5 |     /  
        |   /
    0.0 |__/                   ‚Üê saturated&#33; &#40;gradient ‚âà 0&#41;
        -10  -5   0   5   10
                logit</code></pre>
<p><strong>Why this is bad:</strong></p>
<p>When gates saturate &#40;p ‚âà 0 or p ‚âà 1&#41;:</p>
<ul>
<li><p>Gradients through the sigmoid become tiny: \(\frac{d\sigma}{dx} = \sigma(x)(1-\sigma(x)) \approx 0\)</p>
</li>
<li><p>The gate is &quot;stuck&quot; and hard to update</p>
</li>
<li><p>Training slows down or stops</p>
</li>
</ul>
<p><strong>But wait... isn&#39;t saturation what we want?</strong></p>
<p>Here&#39;s the paradox:</p>
<ul>
<li><p>‚úÖ We <em>want</em> gates to be confidently 0 or 1 &#40;that&#39;s the point&#33;&#41;</p>
</li>
<li><p>‚ùå But we <em>don&#39;t want</em> gradients to vanish &#40;we need to train&#33;&#41;</p>
</li>
</ul>
<h4 id="solution_1_its_actually_fine_with_hard_concrete"><a href="#solution_1_its_actually_fine_with_hard_concrete" class="header-anchor">Solution 1: It&#39;s Actually Fine with Hard Concrete&#33;</a></h4>
<p><strong>Good news:</strong> With Hard Concrete, this is less of a problem than you think&#33;</p>
<p>Here&#39;s why:</p>
<pre><code class="language-julia">function hard_concrete_sample&#40;logit, tau, gamma, zeta&#41;
    u &#61; rand&#40;Float32&#41;
    s &#61; œÉ&#40;&#40;log&#40;u&#41; - log&#40;1-u&#41; &#43; logit&#41; / tau&#41;  # ‚Üê Sigmoid here
    s_bar &#61; s * &#40;zeta - gamma&#41; &#43; gamma
    z &#61; clamp&#40;s_bar, 0, 1&#41;
    return z
end</code></pre>
<p>The gradient flows back through:</p>
<ol>
<li><p>The clamp operation &#40;gradient flows when 0 &lt; s_bar &lt; 1&#41;</p>
</li>
<li><p>The stretching &#40;s_bar &#61; s * &#40;zeta - gamma&#41; &#43; gamma&#41;</p>
</li>
<li><p><strong>The sigmoid with Gumbel noise added&#33;</strong></p>
</li>
</ol>
<p>The key: <code>log&#40;u&#41; - log&#40;1-u&#41;</code> adds random noise that prevents complete saturation&#33; Even if <code>logit</code> is large, the Gumbel noise means <code>s</code> isn&#39;t always saturated.</p>
<h4 id="solution_2_careful_initialization"><a href="#solution_2_careful_initialization" class="header-anchor">Solution 2: Careful Initialization</a></h4>
<p>Don&#39;t initialize gates at extremes:</p>
<pre><code class="language-julia"># BAD: Starts saturated
HardConcrete&#40;init_p&#61;0.999&#41;  # logit ‚âà 6.9, nearly saturated

# GOOD: Starts in the middle
HardConcrete&#40;init_p&#61;0.5&#41;    # logit &#61; 0, maximum gradient

# GOOD: Starts high but not saturated  
HardConcrete&#40;init_p&#61;0.9&#41;    # logit ‚âà 2.2, still has gradient</code></pre>
<p><strong>Rule of thumb:</strong> Initialize p ‚àà &#91;0.3, 0.9&#93; to avoid saturation at the start.</p>
<h4 id="solution_3_alternative_parameterizations"><a href="#solution_3_alternative_parameterizations" class="header-anchor">Solution 3: Alternative Parameterizations</a></h4>
<p>Some papers use different tricks:</p>
<p><strong>a&#41; Bounded logit:</strong></p>
<pre><code class="language-julia">struct BoundedBernoulliGate
    logit_raw::Float32
    logit_min::Float32
    logit_max::Float32
end

function get_logit&#40;gate::BoundedBernoulliGate&#41;
    # Constrain logit to reasonable range
    return gate.logit_min &#43; &#40;gate.logit_max - gate.logit_min&#41; * œÉ&#40;gate.logit_raw&#41;
end

# Example: logit ‚àà &#91;-5, 5&#93; instead of &#40;-‚àû, ‚àû&#41;
gate &#61; BoundedBernoulliGate&#40;logit_raw, -5.0, 5.0&#41;</code></pre>
<p><strong>b&#41; Direct probability parameterization &#40;with softplus&#41;:</strong></p>
<pre><code class="language-julia">struct DirectProbGate
    p_raw::Float32
end

function get_probability&#40;gate::DirectProbGate&#41;
    # Constrain to &#40;0, 1&#41; but avoid hard saturation
    return clamp&#40;œÉ&#40;gate.p_raw&#41;, 0.01, 0.99&#41;
end</code></pre>
<p><strong>c&#41; Temperature scaling &#40;already built-in&#33;&#41;:</strong></p>
<p>Remember that temperature œÑ in the Hard Concrete formula already helps:</p>
<pre><code class="language-julia">s &#61; œÉ&#40;&#40;log&#40;u&#41; - log&#40;1-u&#41; &#43; logit&#41; / tau&#41;</code></pre>
<p>Higher œÑ ‚Üí softer sigmoid ‚Üí less saturation ‚Üí better gradients&#33;</p>
<p>This is why the L0 paper uses œÑ &#61; 2/3 ‚âà 0.67 &#40;relatively high&#41;.</p>
<h4 id="solution_4_regularization_to_prevent_saturation"><a href="#solution_4_regularization_to_prevent_saturation" class="header-anchor">Solution 4: Regularization to Prevent Saturation</a></h4>
<p>Add a regularization term that penalizes extreme logits:</p>
<pre><code class="language-julia">function regularization_loss&#40;gates&#41;
    # Penalize logits that are too extreme
    logit_penalty &#61; sum&#40;abs2, &#91;abs&#40;gate.logit&#41; for gate in gates if abs&#40;gate.logit&#41; &gt; 3&#93;&#41;
    return Œª * logit_penalty
end</code></pre>
<p>But be careful - you still <em>want</em> gates to be decisive eventually&#33;</p>
<h4 id="what_the_l0_paper_actually_does"><a href="#what_the_l0_paper_actually_does" class="header-anchor">What the L0 Paper Actually Does</a></h4>
<p>The L0 regularization paper handles this elegantly:</p>
<ol>
<li><p><strong>Uses sigmoid parameterization</strong> &#40;logit ‚Üí p&#41;</p>
</li>
<li><p><strong>Adds L0 penalty</strong> that encourages gates to close &#40;p ‚Üí 0&#41;</p>
</li>
<li><p><strong>Temperature œÑ &#61; 2/3</strong> provides enough gradient signal</p>
</li>
<li><p><strong>Gumbel noise</strong> prevents complete saturation during training</p>
</li>
<li><p><strong>Accepts that some saturation is okay</strong> - it means confident decisions&#33;</p>
</li>
</ol>
<p>The loss function is: \(\mathcal{L} = \mathcal{L}_{\text{task}} + \lambda \sum_i \sigma(\text{logit}_i - \beta \log(-\gamma/\zeta))\)</p>
<p>The second term explicitly encourages gates to close &#40;p ‚Üí 0&#41;, which means <em>some</em> saturation is actually the goal&#33;</p>
<h4 id="practical_recommendations"><a href="#practical_recommendations" class="header-anchor">Practical Recommendations</a></h4>
<p><strong>For most cases &#40;like L0 regularization&#41;:</strong></p>
<pre><code class="language-julia"># Standard approach - it works&#33;
struct HardConcrete
    logit::Float32  # Learn this with sigmoid&#40;logit&#41; &#61; p
end

# Initialize sensibly
gate &#61; HardConcrete&#40;init_p&#61;0.5&#41;  # or 0.9 for pruning

# Use standard temperature
tau &#61; 2/3</code></pre>
<p><strong>If you&#39;re really worried about saturation:</strong></p>
<pre><code class="language-julia"># Bounded logit version
struct SafeHardConcrete
    logit_raw::Float32
end

function get_logit&#40;gate::SafeHardConcrete&#41;
    # Constrain logit to &#91;-5, 5&#93;
    return 10 * tanh&#40;gate.logit_raw&#41;  # Maps ‚Ñù ‚Üí &#91;-10, 10&#93;
end</code></pre>
<p><strong>Bottom line:</strong> Sigmoid saturation <em>is</em> a concern, but:</p>
<ol>
<li><p>The Gumbel noise helps prevent it during training</p>
</li>
<li><p>Temperature œÑ controls gradient flow</p>
</li>
<li><p>Some saturation is actually desired &#40;confident gates&#33;&#41;</p>
</li>
<li><p>Standard parameterization &#40;unconstrained logit ‚Üí sigmoid&#41; works well in practice</p>
</li>
<li><p>If really concerned, bound the logit or initialize carefully</p>
</li>
</ol>
<p>The L0 paper&#39;s approach has been used successfully in many applications, so the sigmoid parameterization is validated&#33; The key insight: <em>saturation at the end of training is okay</em> - you want confident 0/1 decisions. You just need enough gradient signal <em>during</em> training, which the Gumbel noise and temperature provide.<strong>Bottom line:</strong> œÑ, Œ≥, and Œ∂ are all &quot;meta-parameters&quot; that define the hardness of your sampling mechanism. The network will make all of them softer if given the chance. Keep them fixed and only learn the <code>logit</code> values&#33;</p>
<h3 id="should_p_the_bernoulli_probability_be_adaptive_or_just_a_parameter"><a href="#should_p_the_bernoulli_probability_be_adaptive_or_just_a_parameter" class="header-anchor">Should p &#40;the Bernoulli probability&#41; Be Adaptive or Just a Parameter?</a></h3>
<p>This is actually a <strong>really important design choice</strong> that depends on what you&#39;re trying to do&#33; Let me break down both approaches:</p>
<h4 id="option_1_p_as_a_simple_learnable_parameter_static"><a href="#option_1_p_as_a_simple_learnable_parameter_static" class="header-anchor">Option 1: p as a Simple Learnable Parameter &#40;Static&#41;</a></h4>
<pre><code class="language-julia">struct SimpleBernoulliGate
    logit::Float32  # Just a single number, learned during training
end

Flux.@functor SimpleBernoulliGate

function &#40;gate::SimpleBernoulliGate&#41;&#40;x&#41;
    # Sample once, apply to all inputs
    z &#61; hard_concrete_sample&#40;gate.logit&#41;
    return x .* z
end</code></pre>
<p><strong>Use this when:</strong></p>
<ul>
<li><p>You want a <strong>fixed gate</strong> for a feature/neuron</p>
</li>
<li><p>Like dropout, but learned: &quot;Should this neuron be in the network at all?&quot;</p>
</li>
<li><p>Network pruning: &quot;Is this weight important?&quot;</p>
</li>
<li><p>Feature selection: &quot;Is this input feature useful?&quot;</p>
</li>
</ul>
<p><strong>Example:</strong> L0 regularization for pruning - each weight has its own static probability that&#39;s learned but doesn&#39;t change based on the input.</p>
<h4 id="option_2_p_as_adaptive_input-dependent"><a href="#option_2_p_as_adaptive_input-dependent" class="header-anchor">Option 2: p as Adaptive &#40;Input-Dependent&#41;</a></h4>
<pre><code class="language-julia">struct AdaptiveBernoulliGate
    gate_network::Chain  # Neural network that outputs logit
end

function &#40;gate::AdaptiveBernoulliGate&#41;&#40;x&#41;
    # Compute logit based on input x
    logit &#61; gate.gate_network&#40;x&#41;
    z &#61; hard_concrete_sample&#40;logit&#41;
    return x .* z
end</code></pre>
<p><strong>Use this when:</strong></p>
<ul>
<li><p>You want <strong>context-aware gating</strong></p>
</li>
<li><p>&quot;Should this neuron be active <em>for this particular input</em>?&quot;</p>
</li>
<li><p>Conditional computation: different paths for different inputs</p>
</li>
<li><p>Attention mechanisms: &quot;Which parts of the input matter <em>right now</em>?&quot;</p>
</li>
</ul>
<p><strong>Example:</strong> Adaptive neural networks that activate different parts based on the input &#40;like mixture of experts, or conditional computation&#41;.</p>
<h4 id="the_key_difference"><a href="#the_key_difference" class="header-anchor">The Key Difference</a></h4>
<p><strong>Static p:</strong></p>
<pre><code class="language-julia">Training input 1 ‚Üí p &#61; 0.8 ‚Üí gate
Training input 2 ‚Üí p &#61; 0.8 ‚Üí gate &#40;same&#33;&#41;
Training input 3 ‚Üí p &#61; 0.8 ‚Üí gate &#40;same&#33;&#41;</code></pre>
<p><strong>Adaptive p:</strong></p>
<pre><code class="language-julia">Training input 1 ‚Üí compute p‚ÇÅ &#61; 0.9 ‚Üí gate
Training input 2 ‚Üí compute p‚ÇÇ &#61; 0.3 ‚Üí gate &#40;different&#33;&#41;
Training input 3 ‚Üí compute p‚ÇÉ &#61; 0.7 ‚Üí gate &#40;different&#33;&#41;</code></pre>
<h4 id="comparison_table"><a href="#comparison_table" class="header-anchor">Comparison Table</a></h4>
<table><tr><th align="right">Aspect</th><th align="right">Static p</th><th align="right">Adaptive p</th></tr><tr><td align="right"><strong>Complexity</strong></td><td align="right">Very simple</td><td align="right">More complex</td></tr><tr><td align="right"><strong>Parameters</strong></td><td align="right">1 per gate</td><td align="right">Entire network per gate</td></tr><tr><td align="right"><strong>Computation</strong></td><td align="right">Minimal</td><td align="right">Extra forward passes</td></tr><tr><td align="right"><strong>Use case</strong></td><td align="right">Structural decisions</td><td align="right">Dynamic decisions</td></tr><tr><td align="right"><strong>Examples</strong></td><td align="right">Pruning, feature selection</td><td align="right">Attention, conditional compute</td></tr><tr><td align="right"><strong>Training</strong></td><td align="right">Easier</td><td align="right">Harder &#40;more parameters&#41;</td></tr></table>
<h4 id="code_comparison"><a href="#code_comparison" class="header-anchor">Code Comparison</a></h4>
<p><strong>Static &#40;for pruning&#41;:</strong></p>
<pre><code class="language-julia"># Each neuron has a fixed, learned gate
gates &#61; &#91;HardConcrete&#40;init_p&#61;0.9&#41; for _ in 1:100&#93;

# Forward pass - same gate value regardless of input
function forward&#40;x&#41;
    z &#61; &#91;gate&#40;&#41; for gate in gates&#93;  # Sample once
    return x .* z
end</code></pre>
<p><strong>Adaptive &#40;for conditional computation&#41;:</strong></p>
<pre><code class="language-julia"># Gate depends on the input
struct AdaptiveGate
    gate_net::Dense  # Small network to compute logit
end

function forward&#40;x&#41;
    logit &#61; gate_net&#40;x&#41;  # Different for each x&#33;
    z &#61; hard_concrete_sample&#40;logit&#41;
    return x .* z
end</code></pre>
<h4 id="which_should_you_use"><a href="#which_should_you_use" class="header-anchor">Which Should You Use?</a></h4>
<p><strong>Use static p when:</strong></p>
<ul>
<li><p>‚úÖ You&#39;re doing model compression/pruning</p>
</li>
<li><p>‚úÖ You want to permanently remove features/weights</p>
</li>
<li><p>‚úÖ You want interpretability &#40;which features are important?&#41;</p>
</li>
<li><p>‚úÖ You want efficiency &#40;fewer parameters&#41;</p>
</li>
</ul>
<p><strong>Use adaptive p when:</strong></p>
<ul>
<li><p>‚úÖ You want input-dependent behavior</p>
</li>
<li><p>‚úÖ You&#39;re building attention mechanisms</p>
</li>
<li><p>‚úÖ You need different computation paths for different inputs</p>
</li>
<li><p>‚úÖ You&#39;re okay with extra computational cost</p>
</li>
</ul>
<h4 id="real-world_example_l0_regularization"><a href="#real-world_example_l0_regularization" class="header-anchor">Real-World Example: L0 Regularization</a></h4>
<p>The L0 regularization paper uses <strong>static p</strong> because:</p>
<ul>
<li><p>Goal: Find which weights can be permanently pruned</p>
</li>
<li><p>Each weight gets one learned probability</p>
</li>
<li><p>After training, weights with p ‚âà 0 are removed forever</p>
</li>
<li><p>The decision doesn&#39;t depend on the input</p>
</li>
</ul>
<h4 id="real-world_example_adaptive_computation"><a href="#real-world_example_adaptive_computation" class="header-anchor">Real-World Example: Adaptive Computation</a></h4>
<p>Papers on adaptive depth/width use <strong>adaptive p</strong> because:</p>
<ul>
<li><p>Goal: Use different amounts of computation for different inputs</p>
</li>
<li><p>Easy inputs ‚Üí skip layers &#40;p ‚âà 0 for those layers&#41;</p>
</li>
<li><p>Hard inputs ‚Üí use all layers &#40;p ‚âà 1&#41;</p>
</li>
<li><p>The decision <em>must</em> depend on the input</p>
</li>
</ul>
<p><strong>Bottom line:</strong> Static p is simpler and more common &#40;especially for pruning&#41;. Adaptive p is more powerful but more complex. Choose based on whether your gating decision should be <em>structural</em> &#40;static&#41; or <em>contextual</em> &#40;adaptive&#41;&#33;# The Gumbel-Softmax Trick &#40;But Make It Fun&#41;</p>
<h2 id="so_whats_the_deal"><a href="#so_whats_the_deal" class="header-anchor">So What&#39;s The Deal?</a></h2>
<p>Okay, imagine you&#39;re training a neural network and you need it to make a choice. Like, pick one category out of several options. Maybe you&#39;re building a model that needs to decide &quot;cat, dog, or bird?&quot; for an image.</p>
<p>The natural thing to do is sample from a categorical distribution, right? You have some probabilities \(\pi_1, \pi_2, \ldots, \pi_K\) and you pick one:</p>
\[z = \text{Categorical}(\pi_1, \ldots, \pi_K)\]
<p><strong>Plot twist</strong>: This sampling operation has no gradient. You can&#39;t backpropagate through it. Your optimizer is basically like &quot;¬Ø<br/>&#95;&#40;„ÉÑ&#41;&#95;/¬Ø can&#39;t help you there.&quot;</p>
<p>This is a problem.</p>
<h2 id="enter_the_gumbel-max_trick"><a href="#enter_the_gumbel-max_trick" class="header-anchor">Enter: The Gumbel-Max Trick</a></h2>
<p>Before we get to Gumbel-Softmax, let&#39;s talk about its cooler older sibling, the Gumbel-Max trick.</p>
<p>Here&#39;s a wild fact: if you want to sample from a categorical distribution, you can do this instead:</p>
<ol>
<li><p>Take your log probabilities: \(\alpha_i = \log \pi_i\)</p>
</li>
<li><p>Sample some random Gumbel noise: \(G_i \sim \text{Gumbel}(0, 1)\) for each option</p>
</li>
<li><p>Add them together and pick the max: \(z = \text{one\_hot}\left(\arg\max_i (\alpha_i + G_i)\right)\)</p>
</li>
</ol>
<p>And boom, you&#39;ve sampled from your categorical distribution&#33; It&#39;s mathematically equivalent, which is pretty neat.</p>
<h3 id="quick_aside_whats_a_gumbel_distribution"><a href="#quick_aside_whats_a_gumbel_distribution" class="header-anchor">Quick aside: What&#39;s a Gumbel distribution?</a></h3>
<p>Great question&#33; It&#39;s super easy to sample from:</p>
\[G = -\log(-\log(U)), \quad \text{where } U \sim \text{Uniform}(0, 1)\]
<p>Just sample a uniform random number, take negative log twice, and you&#39;re done. Easy peasy.</p>
<h2 id="the_problem_with_gumbel-max"><a href="#the_problem_with_gumbel-max" class="header-anchor">The Problem With Gumbel-Max</a></h2>
<p>So Gumbel-Max is cool, but we still have that \(\arg\max\) operation, which is... you guessed it... not differentiable. We&#39;re back to square one.</p>
<h2 id="the_gumbel-softmax_solution_the_star_of_the_show"><a href="#the_gumbel-softmax_solution_the_star_of_the_show" class="header-anchor">The Gumbel-Softmax Solution &#40;The Star of the Show&#41;</a></h2>
<p>Here&#39;s the brilliant idea: what if instead of taking the hard \(\arg\max\), we use a <strong>softmax</strong> function? It&#39;s like a &quot;soft&quot; version of max that&#39;s smooth and differentiable&#33;</p>
\[y_i = \frac{\exp((\alpha_i + G_i) / \tau)}{\sum_{j=1}^K \exp((\alpha_j + G_j) / \tau)}\]
<p>where:</p>
<ul>
<li><p>\(\tau\) is the <strong>temperature</strong> &#40;we&#39;ll get to this in a sec&#41;</p>
</li>
<li><p>\(\alpha_i = \log \pi_i\) are your log probabilities &#40;logits&#41;</p>
</li>
<li><p>\(G_i\) is that Gumbel noise we talked about</p>
</li>
</ul>
<p>Instead of getting a one-hot vector like \([0, 0, 1, 0]\), you get something soft like \([0.05, 0.1, 0.8, 0.05]\). And the best part? <strong>Gradients flow through it&#33;</strong></p>
<h2 id="temperature_the_magic_dial"><a href="#temperature_the_magic_dial" class="header-anchor">Temperature: The Magic Dial</a></h2>
<p>The temperature \(\tau\) is like a knob you can turn:</p>
<ul>
<li><p><strong>Low temperature</strong> &#40;\(\tau \to 0\)&#41;: Output becomes spiky, almost one-hot. It&#39;s like &quot;I&#39;m pretty sure it&#39;s option 3.&quot; Gets you closer to true discrete sampling, but gradients get tiny.</p>
</li>
<li><p><strong>High temperature</strong> &#40;\(\tau \to \infty\)&#41;: Output becomes smooth and uniform. It&#39;s like &quot;eh, could be any of them.&quot; Better gradients, but less decisive.</p>
</li>
<li><p><strong>Sweet spot</strong>: Usually people use \(\tau \in [0.1, 1]\) during training. Sometimes you even anneal &#40;gradually decrease&#41; it over time.</p>
</li>
</ul>
<h3 id="can_temperature_be_learned"><a href="#can_temperature_be_learned" class="header-anchor">Can Temperature Be Learned?</a></h3>
<p>Great question&#33; The short answer: <strong>Yes, but people rarely do it, and here&#39;s why:</strong></p>
<p><strong>The problem with learning \(\tau\):</strong></p>
<ul>
<li><p>The network will probably learn to make \(\tau\) really large to avoid making hard decisions &#40;gradients are easier when everything is smooth&#33;&#41;</p>
</li>
<li><p>It&#39;s kinda like asking the network &quot;how confident do you want to be?&quot; and the network answering &quot;not confident at all, that&#39;s easier&quot;</p>
</li>
<li><p>You&#39;d need to add regularization or constraints to prevent this</p>
</li>
</ul>
<p><strong>What people actually do:</strong></p>
<ol>
<li><p><strong>Fixed temperature</strong>: Just pick a constant like \(\tau = 0.5\) or \(\tau = 1.0\). Simple and works fine.</p>
</li>
<li><p><strong>Temperature annealing</strong> &#40;most common&#41;: Start with high temperature for better gradients early on, then gradually decrease it:</p>
</li>
</ol>
<pre><code class="language-python"># Example annealing schedule
   tau &#61; max&#40;0.5, exp&#40;-anneal_rate * step&#41;&#41;</code></pre>
<p>This is like training wheels - smooth gradients at first, then more decisive later.</p>
<ol start="3">
<li><p><strong>Per-layer learned temperature</strong> &#40;advanced&#41;: In some neural architecture search papers, they DO learn temperature, but with tricks:</p>
<ul>
<li><p>Initialize it to a reasonable value</p>
</li>
<li><p>Constrain it: \(\tau = \text{softplus}(\tau_{\text{raw}}) + \epsilon\) to keep it positive and bounded</p>
</li>
<li><p>Add a penalty for high temperatures in the loss</p>
</li>
</ul>
</li>
<li><p><strong>Adaptive schedules</strong>: Some papers use heuristics that adjust \(\tau\) based on training progress or validation performance.</p>
</li>
</ol>
<h3 id="the_hard_concrete_distribution_trick"><a href="#the_hard_concrete_distribution_trick" class="header-anchor">The Hard Concrete Distribution Trick</a></h3>
<p>Now here&#39;s a really clever approach from the <strong>L0 regularization paper</strong> you mentioned&#33; Instead of dealing with temperature, they use a different strategy for the Binary Concrete case:</p>
<p><strong>The idea</strong>: Transform your soft sample \(s\) &#40;which is in \((0, 1)\)&#41; to be &quot;hard&quot; by stretching and clamping it:</p>
\(\bar{z} = \min(1, \max(0, s \cdot (\zeta - \gamma) + \gamma))\)
<p>where:</p>
<ul>
<li><p>\(s\) is your soft sample from Binary Concrete &#40;using some temperature&#41;</p>
</li>
<li><p>\(\gamma < 0\) &#40;like \(-0.1\)&#41; is the left stretch</p>
</li>
<li><p>\(\zeta > 1\) &#40;like \(1.1\)&#41; is the right stretch</p>
</li>
<li><p>\(\bar{z}\) is the &quot;hardened&quot; output</p>
</li>
</ul>
<p><strong>What&#39;s happening here?</strong> By setting \(\gamma < 0\) and \(\zeta > 1\), you&#39;re stretching the \((0,1)\) interval beyond its bounds, then clamping. This pushes values closer to 0 or 1&#33;</p>
<p><strong>Visual intuition:</strong></p>
<pre><code class="language-julia">Before stretching: s ‚àà &#40;0, 1&#41;
         &#91;0 &#61;&#61;&#61;&#61;&#61;&#61;&#61;&#61;&#61;&#61; 1&#93;
         
After stretch &amp; clamp: s * 1.2 - 0.1
    &#91;-0.1 &#61;&#61;&#61;&#61;&#61;&#61;&#61;&#61;&#61;&#61;&#61;&#61; 1.1&#93;
    clip to &#91;0, 1&#93;
    
Result: Most values pushed to 0 or 1&#33;</code></pre>
<p><strong>In practice for L0 regularization:</strong></p>
<pre><code class="language-python">def hard_concrete_sample&#40;logit, tau&#61;2/3, gamma&#61;-0.1, zeta&#61;1.1&#41;:
    &quot;&quot;&quot;
    Hard Concrete distribution &#40;L0 regularization paper&#41;
    &quot;&quot;&quot;
    # Sample soft gate
    u &#61; uniform&#40;0, 1&#41;
    s &#61; sigmoid&#40;&#40;log&#40;u&#41; - log&#40;1-u&#41; &#43; logit&#41; / tau&#41;
    
    # Stretch and clamp &#40;&quot;hard&quot; version&#41;
    s_bar &#61; s * &#40;zeta - gamma&#41; &#43; gamma
    z &#61; min&#40;1, max&#40;0, s_bar&#41;&#41;
    
    return z</code></pre>
<p><strong>Julia implementation with learnable parameters:</strong></p>
<pre><code class="language-julia">using Flux
using Random

struct HardConcrete&#123;T&#125;
    logit::T  # Learnable parameter &#40;log-odds&#41;
    tau::Float32
    gamma::Float32
    zeta::Float32
end

# Constructor with learnable logit
function HardConcrete&#40;;init_p&#61;0.5, tau&#61;2/3, gamma&#61;-0.1, zeta&#61;1.1&#41;
    # Convert probability p to logit: log&#40;p / &#40;1-p&#41;&#41;
    logit &#61; log&#40;init_p / &#40;1 - init_p&#41;&#41;
    HardConcrete&#40;logit, Float32&#40;tau&#41;, Float32&#40;gamma&#41;, Float32&#40;zeta&#41;&#41;
end

# Make it Flux-compatible for automatic differentiation
Flux.@functor HardConcrete

# Sampling function
function &#40;hc::HardConcrete&#41;&#40;&#41;
    # Sample uniform noise
    u &#61; rand&#40;Float32&#41;
    
    # Compute soft sample using Gumbel-Softmax/Binary Concrete
    s &#61; œÉ&#40;&#40;log&#40;u&#41; - log&#40;1 - u&#41; &#43; hc.logit&#41; / hc.tau&#41;
    
    # Stretch and clamp to get hard sample
    s_bar &#61; s * &#40;hc.zeta - hc.gamma&#41; &#43; hc.gamma
    z &#61; clamp&#40;s_bar, 0f0, 1f0&#41;
    
    return z
end

# Helper to get the current probability
function get_probability&#40;hc::HardConcrete&#41;
    return œÉ&#40;hc.logit&#41;
end

# Example usage
function example_usage&#40;&#41;
    # Create a learnable Hard Concrete gate starting at p&#61;0.8
    gate &#61; HardConcrete&#40;init_p&#61;0.8&#41;
    
    # Sample from it
    sample &#61; gate&#40;&#41;
    println&#40;&quot;Sample: &quot;, sample&#41;
    println&#40;&quot;Current p: &quot;, get_probability&#40;gate&#41;&#41;
    
    # Use in a neural network
    model &#61; Chain&#40;
        Dense&#40;10, 20&#41;,
        x -&gt; x .* gate&#40;&#41;,  # Apply learned gate
        Dense&#40;20, 1&#41;
    &#41;
    
    # The gate.logit will be learned during training&#33;
    params &#61; Flux.params&#40;model, gate&#41;
    
    return model, gate
end</code></pre>
<h3 id="should_you_learn_the_stretching_parameters_Œ≥_Œ∂"><a href="#should_you_learn_the_stretching_parameters_Œ≥_Œ∂" class="header-anchor">Should You Learn the Stretching Parameters &#40;Œ≥, Œ∂&#41;?</a></h3>
<p><strong>Short answer: NO, keep them fixed&#33;</strong> Here&#39;s why:</p>
<p><strong>The stretching parameters Œ≥ and Œ∂ control the &quot;hardness&quot;:</strong></p>
<ul>
<li><p>They determine how aggressively values get pushed to 0 or 1</p>
</li>
<li><p>Œ≥ &lt; 0 and Œ∂ &gt; 1 create the stretching effect</p>
</li>
<li><p>Standard values: Œ≥ &#61; -0.1, Œ∂ &#61; 1.1 &#40;from the L0 paper&#41;</p>
</li>
</ul>
<p><strong>Why NOT learn them:</strong></p>
<p><strong>YES, exactly&#33; The network will make them &quot;loose&quot; &#40;soft&#41;.</strong> Here&#39;s the intuition:</p>
<p>Think about what the network &quot;wants&quot; during training:</p>
<ul>
<li><p><strong>Hard gates</strong> &#40;Œ≥ very negative, Œ∂ very positive&#41;: Clear 0s and 1s, but small gradients through them</p>
</li>
<li><p><strong>Soft gates</strong> &#40;Œ≥ ‚Üí 0, Œ∂ ‚Üí 1&#41;: Smooth values, easy gradients, easier optimization</p>
</li>
</ul>
<p>The network&#39;s gradient descent will push toward: <strong>&quot;Whatever makes my loss go down easiest&quot;</strong></p>
<p>And what makes optimization easiest? <strong>Soft, smooth functions&#33;</strong></p>
<p>So if you let the network learn Œ≥ and Œ∂, it would likely do this:</p>
<pre><code class="language-julia">Initial:  Œ≥ &#61; -0.1,  Œ∂ &#61; 1.1  &#40;nice and hard&#41;
          ‚Üì gradient descent
After:    Œ≥ &#61; -0.01, Œ∂ &#61; 1.01 &#40;getting softer...&#41;
          ‚Üì gradient descent  
Final:    Œ≥ ‚âà 0,    Œ∂ ‚âà 1    &#40;basically no stretching&#33;&#41;</code></pre>
<p><strong>Why does this happen?</strong></p>
<ol>
<li><p><strong>Gradients are bigger through soft functions</strong> - calculus loves smooth curves&#33;</p>
</li>
<li><p><strong>No explicit pressure for hardness</strong> - your loss function doesn&#39;t say &quot;make it hard&quot;, it just says &quot;classify correctly&quot;</p>
</li>
<li><p><strong>The path of least resistance</strong> - soft is easier, so SGD goes there</p>
</li>
</ol>
<p><strong>Analogy:</strong> It&#39;s like asking a student to set their own exam difficulty. They&#39;ll make it as easy as possible&#33; You need the teacher &#40;you, the researcher&#41; to set the difficulty externally.</p>
<p><strong>The fix:</strong> Keep Œ≥, Œ∂ fixed at values that give you the hardness you want. Let the network only learn the <code>logit</code> values &#40;which gates to open/close&#41;.</p>
<p><strong>Could you force it to stay hard?</strong> Technically yes, with tricks:</p>
<ul>
<li><p>Add regularization: penalize Œ≥ and Œ∂ for being too close to 0 and 1</p>
</li>
<li><p>Use hard constraints: clip them or use specific parameterizations</p>
</li>
<li><p>Add a &quot;hardness loss&quot; to explicitly encourage hard gates</p>
</li>
</ul>
<p>But this is way more complicated than just... keeping them fixed&#33; üòÖ</p>
<p><strong>What about temperature œÑ?</strong></p>
<p><strong>Same problem, same answer: Keep it fixed too&#33;</strong></p>
<p>Temperature œÑ controls the softness in a different way - it&#39;s in the sampling step <em>before</em> stretching:</p>
\(s = \sigma\left(\frac{\log(u) - \log(1-u) + \text{logit}}{\tau}\right)\)
<p><strong>If you let the network learn œÑ:</strong></p>
<ul>
<li><p><strong>High œÑ</strong> &#40;like œÑ&#61;10&#41;: Softmax becomes very smooth ‚Üí easy gradients ‚Üí network prefers this&#33;</p>
</li>
<li><p><strong>Low œÑ</strong> &#40;like œÑ&#61;0.1&#41;: Softmax becomes spiky ‚Üí harder gradients ‚Üí network avoids this&#33;</p>
</li>
</ul>
<p>So the network will push œÑ higher and higher to make optimization easier.</p>
<p><strong>The progression would be:</strong></p>
<pre><code class="language-julia">Initial:  œÑ &#61; 0.67  &#40;reasonable&#41;
          ‚Üì gradient descent
After:    œÑ &#61; 2.0   &#40;getting softer...&#41;
          ‚Üì gradient descent  
Final:    œÑ &#61; 10&#43;   &#40;way too soft, basically uniform&#33;&#41;</code></pre>
<p><strong>Why all three &#40;œÑ, Œ≥, Œ∂&#41; want to go &quot;loose&quot;:</strong></p>
<p>All three parameters control hardness in different ways, but they all face the same issue:</p>
<table><tr><th align="right">Parameter</th><th align="right">What it does</th><th align="right">What network wants</th></tr><tr><td align="right"><strong>œÑ</strong></td><td align="right">Controls softmax sharpness</td><td align="right">œÑ ‚Üí ‚àû &#40;very soft&#41;</td></tr><tr><td align="right"><strong>Œ≥</strong></td><td align="right">Left stretch boundary</td><td align="right">Œ≥ ‚Üí 0 &#40;less stretch&#41;</td></tr><tr><td align="right"><strong>Œ∂</strong></td><td align="right">Right stretch boundary</td><td align="right">Œ∂ ‚Üí 1 &#40;less stretch&#41;</td></tr></table>
<p>All arrows point toward <strong>&quot;make it easier to optimize&quot;</strong>&#33;</p>
<p><strong>Standard practice for all three:</strong></p>
<pre><code class="language-julia">struct HardConcrete&#123;T&#125;
    logit::T              # ‚úÖ LEARN THIS
    tau::Float32          # üîí KEEP FIXED &#40;e.g., 2/3&#41;
    gamma::Float32        # üîí KEEP FIXED &#40;e.g., -0.1&#41;
    zeta::Float32         # üîí KEEP FIXED &#40;e.g., 1.1&#41;
end</code></pre>
<p><strong>Only mark logit as learnable:</strong></p>
<pre><code class="language-julia"># This tells Flux to only train the logit
Flux.@functor HardConcrete
Flux.trainable&#40;hc::HardConcrete&#41; &#61; &#40;logit &#61; hc.logit,&#41;</code></pre>
<p><strong>Bottom line:</strong> œÑ, Œ≥, and Œ∂ are all &quot;meta-parameters&quot; that define the hardness of your sampling mechanism. The network will make all of them softer if given the chance. Keep them fixed and only learn the <code>logit</code> values&#33;</p>
<p><strong>What you COULD learn &#40;but usually don&#39;t&#41;:</strong></p>
<p>If you really wanted to, you could make œÑ &#40;temperature&#41; learnable <em>per-layer</em> with constraints:</p>
<pre><code class="language-julia">struct HardConcreteWithLearnableTau&#123;T&#125;
    logit::T
    tau_raw::T  # Unconstrained
    gamma::Float32
    zeta::Float32
end

function &#40;hc::HardConcreteWithLearnableTau&#41;&#40;&#41;
    # Constrain tau to reasonable range, e.g., &#91;0.1, 2.0&#93;
    tau &#61; 0.1 &#43; 1.9 * œÉ&#40;hc.tau_raw&#41;
    
    u &#61; rand&#40;Float32&#41;
    s &#61; œÉ&#40;&#40;log&#40;u&#41; - log&#40;1 - u&#41; &#43; hc.logit&#41; / tau&#41;
    s_bar &#61; s * &#40;hc.zeta - hc.gamma&#41; &#43; hc.gamma
    z &#61; clamp&#40;s_bar, 0f0, 1f0&#41;
    
    return z
end</code></pre>
<p>But even this is rare in practice. Most papers stick with fixed hyperparameters.</p>
<p><strong>Bottom line:</strong> Learn the <code>logit</code> &#40;which controls the probability p&#41;, but keep Œ≥, Œ∂, and œÑ fixed. They&#39;re &quot;meta-parameters&quot; that define the sampling mechanism, not model parameters that should adapt to data.</p>
<p><strong>For a layer with multiple gates &#40;like L0 regularization for each weight&#41;:</strong></p>
<pre><code class="language-julia">struct HardConcreteLayer&#123;T&#125;
    logits::T  # Vector of learnable logits, one per feature
    tau::Float32
    gamma::Float32
    zeta::Float32
end

function HardConcreteLayer&#40;n_features::Int; init_p&#61;0.5, tau&#61;2/3, gamma&#61;-0.1, zeta&#61;1.1&#41;
    logit_val &#61; log&#40;init_p / &#40;1 - init_p&#41;&#41;
    logits &#61; fill&#40;Float32&#40;logit_val&#41;, n_features&#41;
    HardConcreteLayer&#40;logits, Float32&#40;tau&#41;, Float32&#40;gamma&#41;, Float32&#40;zeta&#41;&#41;
end

Flux.@functor HardConcreteLayer

function &#40;hcl::HardConcreteLayer&#41;&#40;&#41;
    # Sample uniform noise for all features at once
    u &#61; rand&#40;Float32, length&#40;hcl.logits&#41;&#41;
    
    # Vectorized computation
    s &#61; œÉ.&#40;&#40;log.&#40;u&#41; .- log.&#40;1 .- u&#41; .&#43; hcl.logits&#41; ./ hcl.tau&#41;
    s_bar &#61; s .* &#40;hcl.zeta - hcl.gamma&#41; .&#43; hcl.gamma
    z &#61; clamp.&#40;s_bar, 0f0, 1f0&#41;
    
    return z
end

# Get sparsity &#40;how many gates are closed&#41;
function get_sparsity&#40;hcl::HardConcreteLayer&#41;
    probs &#61; œÉ.&#40;hcl.logits&#41;
    return 1 - mean&#40;probs&#41;
end

# Example: Sparse neural network layer
function sparse_layer_example&#40;&#41;
    # 100 features, each with learnable gate
    gates &#61; HardConcreteLayer&#40;100, init_p&#61;0.9&#41;  # Start mostly open
    
    # During forward pass
    z &#61; gates&#40;&#41;  # Sample binary gates
    
    # Apply gates to activations
    # x &#61; weights * input
    # output &#61; x .* z  # Element-wise masking
    
    println&#40;&quot;Sparsity: &quot;, get_sparsity&#40;gates&#41;, &quot;&#37;&quot;&#41;
    println&#40;&quot;Active features: &quot;, sum&#40;z&#41;&#41;
    
    return gates
end</code></pre>
<p><strong>Key differences from Python version:</strong></p>
<ul>
<li><p><code>logit</code> is now a <strong>learnable parameter</strong> that Flux will optimize</p>
</li>
<li><p>Use <code>Flux.@functor</code> to tell Flux which fields are trainable</p>
</li>
<li><p>Can create single gate or layer of gates</p>
</li>
<li><p>The probability <code>p</code> is implicitly learned through <code>logit &#61; log&#40;p/&#40;1-p&#41;&#41;</code></p>
</li>
<li><p>During training, gradients will flow back to update the logits&#33;</p>
</li>
</ul>
<p><strong>During training:</strong></p>
<ul>
<li><p><strong>Forward pass</strong>: Use the hard \(\bar{z}\) &#40;which is often exactly 0 or 1 after clamping&#41;</p>
</li>
<li><p><strong>Backward pass</strong>: Gradient flows through the soft \(s\) before the clamp</p>
</li>
</ul>
<p><strong>Why this is brilliant for sparse networks:</strong></p>
<ul>
<li><p>You get actual hard 0/1 gates that <em>truly</em> remove weights &#40;sparsity&#33;&#41;</p>
</li>
<li><p>Still differentiable through the soft \(s\)</p>
</li>
<li><p>No need to anneal temperature or worry about the network &quot;cheating&quot;</p>
</li>
<li><p>The stretch parameters \(\gamma, \zeta\) control how hard the gates are</p>
</li>
</ul>
<p><strong>Comparison to straight-through:</strong></p>
<ul>
<li><p>Straight-through: \(\text{forward} = \text{one\_hot}(\arg\max)\), backward through soft values</p>
</li>
<li><p>Hard Concrete: \(\text{forward} = \text{stretched & clamped}\), backward through soft values</p>
</li>
</ul>
<p>Both achieve &quot;hard in forward, soft in backward&quot; but Hard Concrete is smoother and more principled&#33;</p>
<p><strong>Bottom line</strong>: You <em>can</em> make it learnable, but you need to be careful or the network will &quot;cheat&quot; by keeping everything soft. Fixed or annealed schedules are usually simpler and work just as well. OR, use the Hard Concrete trick to get hard samples without worrying about temperature at all&#33;</p>
<h2 id="two_flavors"><a href="#two_flavors" class="header-anchor">Two Flavors</a></h2>
<h3 id="flavor_1_straight-through_estimator"><a href="#flavor_1_straight-through_estimator" class="header-anchor">Flavor 1: Straight-Through Estimator</a></h3>
<p>This one&#39;s sneaky:</p>
<ul>
<li><p><strong>Forward pass</strong>: Sample discrete one-hot vectors \(z = \text{one\_hot}(\arg\max_i y_i)\)</p>
</li>
<li><p><strong>Backward pass</strong>: Pretend you used the continuous \(y\) and pass those gradients</p>
</li>
</ul>
<p>It&#39;s like lying to the backward pass, but in a good way&#33; You get discrete samples where you need them, but gradients flow as if everything was continuous.</p>
<h3 id="flavor_2_fully_continuous"><a href="#flavor_2_fully_continuous" class="header-anchor">Flavor 2: Fully Continuous</a></h3>
<p>Just use the soft vector \(y\) everywhere. No tricks, just smooth continuous outputs throughout.</p>
<h2 id="but_wait_what_about_bernoulli_outputs"><a href="#but_wait_what_about_bernoulli_outputs" class="header-anchor">But Wait, What About Bernoulli Outputs?</a></h2>
<p>Ah, great question&#33; What if you just need a binary choice? Like yes/no, 0/1, true/false?</p>
<p>Good news: <strong>Bernoulli is just a special case&#33;</strong> A Bernoulli distribution is really just a categorical distribution with \(K=2\) classes.</p>
<p>Here&#39;s how you&#39;d do it:</p>
<h3 id="method_1_use_2-class_gumbel-softmax"><a href="#method_1_use_2-class_gumbel-softmax" class="header-anchor">Method 1: Use 2-Class Gumbel-Softmax</a></h3>
<pre><code class="language-python"># Say you have probability p of class 1
logits &#61; &#91;log&#40;1-p&#41;, log&#40;p&#41;&#93;  # logits for &#91;class 0, class 1&#93;

# Apply Gumbel-Softmax
y &#61; gumbel_softmax&#40;logits, tau&#41;  # returns &#91;y_0, y_1&#93;

# If you want a single number, just take the second component
binary_soft &#61; y&#91;1&#93;  # this is approximately 0 or 1</code></pre>
<h3 id="method_2_binary_concrete_distribution"><a href="#method_2_binary_concrete_distribution" class="header-anchor">Method 2: Binary Concrete Distribution</a></h3>
<p>There&#39;s actually a specific name for Bernoulli with Gumbel-Softmax: the <strong>Binary Concrete Distribution</strong> &#40;also called BinConcrete&#41;.</p>
<p>For a Bernoulli with probability \(p\), you can directly sample:</p>
\[y = \frac{1}{1 + \exp(-((\log p - \log(1-p) + G_1 - G_0) / \tau))}\]
<p>where \(G_0, G_1 \sim \text{Gumbel}(0,1)\) are independent.</p>
<p>This simplifies to:</p>
\[y = \sigma\left(\frac{\log p - \log(1-p) + G_1 - G_0}{\tau}\right)\]
<p>where \(\sigma\) is the sigmoid function. This gives you a number in \((0, 1)\) that&#39;s approximately 0 or 1 when \(\tau\) is small.</p>
<h3 id="practical_code_for_bernoulli"><a href="#practical_code_for_bernoulli" class="header-anchor">Practical Code for Bernoulli</a></h3>
<pre><code class="language-python">def binary_concrete_sample&#40;logit, tau&#61;1.0&#41;:
    &quot;&quot;&quot;
    Sample from Binary Concrete &#40;Gumbel-Softmax for Bernoulli&#41;
    
    Args:
        logit: log&#40;p / &#40;1-p&#41;&#41; where p is Bernoulli probability
        tau: temperature
    
    Returns:
        Soft binary sample in &#40;0, 1&#41;
    &quot;&quot;&quot;
    U1, U2 &#61; uniform&#40;0, 1&#41;, uniform&#40;0, 1&#41;
    G1, G2 &#61; -log&#40;-log&#40;U1&#41;&#41;, -log&#40;-log&#40;U2&#41;&#41;
    
    return sigmoid&#40;&#40;logit &#43; G1 - G2&#41; / tau&#41;</code></pre>
<p>For straight-through:</p>
<pre><code class="language-python"># Forward: hard binary
z &#61; &#40;y &gt; 0.5&#41;.float&#40;&#41;  # or round&#40;y&#41;

# Backward: use gradients from soft y</code></pre>
<h2 id="real-world_applications"><a href="#real-world_applications" class="header-anchor">Real-World Applications</a></h2>
<p>People use this trick for:</p>
<ol>
<li><p><strong>VAEs with discrete latents</strong> - &quot;Should this pixel be in cluster A or B?&quot;</p>
</li>
<li><p><strong>Neural architecture search</strong> - &quot;Should I use a conv layer or skip connection here?&quot;</p>
</li>
<li><p><strong>Reinforcement learning</strong> - &quot;Which action should the agent take?&quot;</p>
</li>
<li><p><strong>Attention mechanisms</strong> - &quot;Which part of the input should I focus on?&quot;</p>
</li>
<li><p><strong>Binary gates</strong> - &quot;Should this neuron be active or not?&quot; &#40;your Bernoulli case&#33;&#41;</p>
</li>
</ol>
<h2 id="why_this_trick_is_awesome"><a href="#why_this_trick_is_awesome" class="header-anchor">Why This Trick Is Awesome</a></h2>
<ul>
<li><p>‚úÖ <strong>It just works</strong> - Drop it into your model and start training</p>
</li>
<li><p>‚úÖ <strong>Lower variance</strong> - Better than methods like REINFORCE</p>
</li>
<li><p>‚úÖ <strong>Unbiased</strong> - As \(\tau \to 0\), you recover the true distribution</p>
</li>
<li><p>‚úÖ <strong>Simple</strong> - Like 5 lines of code</p>
</li>
<li><p>‚úÖ <strong>Flexible</strong> - Works for any categorical distribution &#40;including Bernoulli&#33;&#41;</p>
</li>
</ul>
<h2 id="quick_example_discrete_vae"><a href="#quick_example_discrete_vae" class="header-anchor">Quick Example: Discrete VAE</a></h2>
<p>Without Gumbel-Softmax:</p>
<pre><code class="language-julia">Encoder ‚Üí probabilities ‚Üí &#91;SAMPLING - NOT DIFFERENTIABLE&#33;&#93; ‚Üí Decoder
                                ‚ùå Gradients die here</code></pre>
<p>With Gumbel-Softmax:</p>
<pre><code class="language-julia">Encoder ‚Üí logits ‚Üí Gumbel-Softmax ‚Üí soft samples ‚Üí Decoder
                         ‚úÖ Gradients flow&#33;</code></pre>
<p>And that&#39;s it&#33; You can now backpropagate through discrete choices. Magic&#33; üéâ</p>
<h2 id="tldr"><a href="#tldr" class="header-anchor">TL;DR</a></h2>
<p>Need to sample discrete choices in a neural network? Can&#39;t backprop through sampling? Add Gumbel noise, apply softmax with temperature, and you&#39;ve got yourself a differentiable approximation. Works for categorical distributions &#40;pick 1 of K&#41; and Bernoulli distributions &#40;binary choice&#41; alike&#33;</p>
<div class="page-foot">
    Contact me by <a href="mailto:skchu@wustl.edu">E-mail</a> | <a href="https://github.com/kchu25">Github</a> | <a href="https://www.linkedin.com/in/kchu1/">Linkedin</a>
    <br>
    This work is licensed under <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a>.  Last modified: November 26, 2025.
    <br>
    Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia language</a>.
</div>
</div><!-- CONTENT ENDS HERE -->
    
        <script src="/libs/katex/katex.min.js"></script>
<script src="/libs/katex/contrib/auto-render.min.js"></script>
<script>renderMathInElement(document.body)</script>

    
    
        <script src="/libs/highlight/highlight.min.js"></script>
<script>hljs.highlightAll();hljs.configure({tabReplace: '    '});</script>

    
  </body>
</html>
