<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  
  <link rel="stylesheet" href="/css/franklin.css">
<link rel="stylesheet" href="/css/hypertext.css">
<link rel="icon" href="/assets/favicon.png">

   <title>schu - Research notes</title>  
</head>
<body>
<header>
  <h1 style="color:#283747">computer science</h1>
  <nav>
    <a href="/" class="current">Tags</a>
  | <a href="/blog/" >Notes</a>
  <hr/>
  </nav>
</header>


<!-- Content appended here -->
<div class="franklin-content">
<h1>Posts</h1>

<ul class="blog-posts"><li><span><i>2026-01-03  </i></span>&emsp;<a href="/blog/conc_ineq/chernoff/">Step-by-Step Guide: Using Chernoff Bounds</a><li><span><i>2025-12-29  </i></span>&emsp;<a href="/blog/OS/os_from_bad_undergrad/">Core Mental Models for Operating Systems</a><li><span><i>2025-12-29  </i></span>&emsp;<a href="/blog/interpretation/modisco/">Understanding Seqlets in TF-MoDISco</a><li><span><i>2025-12-20  </i></span>&emsp;<a href="/blog/interpretation/additive/">Interpretation Frameworks Using Additive Decomposition</a><li><span><i>2025-12-20  </i></span>&emsp;<a href="/blog/interpretation/deeplift/">DeepLIFT Notation: Understanding the Subscripts</a><li><span><i>2025-12-20  </i></span>&emsp;<a href="/blog/interpretation/efficiency/">Why Efficiency Matters in Explainability</a><li><span><i>2025-12-20  </i></span>&emsp;<a href="/blog/interpretation/shap_additive/">DeepSHAP: Additive Decomposition and DeepLIFT Connection</a><li><span><i>2025-12-20  </i></span>&emsp;<a href="/blog/interpretation/sum_deepshap/">Why the Sum Disappears in DeepSHAP</a><li><span><i>2025-12-19  </i></span>&emsp;<a href="/blog/interpretation/cond_indep_shap/">The Conditional Independence Assumption in DeepSHAP</a><li><span><i>2025-12-19  </i></span>&emsp;<a href="/blog/interpretation/deepleft2shapley/">From DeepLIFT Contributions to Shapley Values: The Quantitative Connection</a><li><span><i>2025-12-19  </i></span>&emsp;<a href="/blog/interpretation/deepshap_indep/">Understanding SHAP's Independence Assumption</a><li><span><i>2025-12-19  </i></span>&emsp;<a href="/blog/interpretation/deepshap_intuitive/">Making the SHAP Approximation Intuitive</a><li><span><i>2025-12-19  </i></span>&emsp;<a href="/blog/interpretation/deepshap_notation/">Understanding DeepSHAP Notation and Reference Averaging</a><li><span><i>2025-12-19  </i></span>&emsp;<a href="/blog/interpretation/key_eqns/">SHAP: Key Equations in Aas, Jullum, and Løland (2021)</a><li><span><i>2025-12-19  </i></span>&emsp;<a href="/blog/interpretation/shap_ref_and_marginal/">Why the DeepSHAP Double Expectation Formula Holds</a><li><span><i>2025-12-19  </i></span>&emsp;<a href="/blog/interpretation/tracing_shap/">Tracing DeepSHAP to Lundberg's Original Paper</a><li><span><i>2025-12-10  </i></span>&emsp;<a href="/blog/general_topics/additivity/">Additivity and Computational Complexity</a><li><span><i>2025-12-08  </i></span>&emsp;<a href="/blog/conc_ineq/conc_design/">Design Patterns for Concentration Inequalities in Sample Complexity</a><li><span><i>2025-12-08  </i></span>&emsp;<a href="/blog/conc_ineq/landau/">Understanding Landau Notation in Concentration Inequalities</a><li><span><i>2025-12-08  </i></span>&emsp;<a href="/blog/conc_ineq/log_sample/">Understanding ε² and Logarithmic Sample Complexity</a><li><span><i>2025-12-08  </i></span>&emsp;<a href="/blog/conc_ineq/understand_freq/">Understanding "Frequency" in ε² and Logarithmic Sample Complexity</a><li><span><i>2025-12-08  </i></span>&emsp;<a href="/blog/machine-learning/filter_diversity/">Novel Methods for CNN Filter Diversity</a><li><span><i>2025-12-07  </i></span>&emsp;<a href="/blog/conc_ineq/config_conc2/">Uniform Sampling with Concentration Bounds for Configuration Enumeration - II</a><li><span><i>2025-12-05  </i></span>&emsp;<a href="/blog/conc_ineq/config_conc/">Uniform Sampling with Concentration Bounds for Configuration Enumeration</a><li><span><i>2025-12-01  </i></span>&emsp;<a href="/blog/machine-learning/ste/">Straight-Through Estimator (STE)</a><li><span><i>2025-11-27  </i></span>&emsp;<a href="/blog/block_chains/block_chain_demo/">Building a Simple Blockchain in Julia</a><li><span><i>2025-11-26  </i></span>&emsp;<a href="/blog/machine-learning/gumbel_softmax_trick/">The Gumbel-Softmax Trick </a><li><span><i>2025-11-22  </i></span>&emsp;<a href="/blog/machine-learning/eff_net/">EfficientNet: Compound Scaling & MBConv Blocks - Complete Guide</a><li><span><i>2025-11-16  </i></span>&emsp;<a href="/blog/machine-learning/regularization_at_layer/">Regularizing Final vs First Layer Embeddings</a><li><span><i>2025-11-05  </i></span>&emsp;<a href="/blog/machine-learning/flat_minima/">Unconventional Approaches to Interpretability in Flat Minima</a><li><span><i>2025-11-05  </i></span>&emsp;<a href="/blog/machine-learning/sharpness_aware/">Sharpness-Aware Minimization (SAM) - A Mathematical & Intuitive Guide</a><li><span><i>2025-11-04  </i></span>&emsp;<a href="/blog/data_processing/log_transform_tradeoffs/">Understanding Log Transform Trade-offs in Regression</a><li><span><i>2025-11-04  </i></span>&emsp;<a href="/blog/machine-learning/loss_log_scale/">Loss Functions for Log-Scale Regression</a><li><span><i>2025-11-03  </i></span>&emsp;<a href="/blog/data_processing/log_normalize/">Log Min-Max Normalization</a><li><span><i>2025-11-02  </i></span>&emsp;<a href="/blog/data_processing/normalization/">Neural Network Output Activations for Normalized Labels</a><li><span><i>2025-11-01  </i></span>&emsp;<a href="/blog/machine-learning/group_equivariance/">Group Equivariance in Neural Networks</a><li><span><i>2025-11-01  </i></span>&emsp;<a href="/blog/machine-learning/spatial_locality/">Spatial Locality as Inductive Bias</a><li><span><i>2025-10-28  </i></span>&emsp;<a href="/blog/machine-learning/sparsity_trade_off/">Is the Sparsity-Accuracy Tradeoff Worth It?</a><li><span><i>2025-10-11  </i></span>&emsp;<a href="/blog/deep_generative_models/diffusion/">Introduction to Diffusion Models</a><li><span><i>2025-10-10  </i></span>&emsp;<a href="/blog/sparsity/learn_l0/">Learning Sparse Neural Networks Through L0 Regularization</a><li><span><i>2025-10-09  </i></span>&emsp;<a href="/blog/machine-learning/gnn/">Graph Neural Networks: Step-by-Step with Simple Examples</a><li><span><i>2025-10-09  </i></span>&emsp;<a href="/blog/probabilistic_models/bayesian_net/">A Conversational Guide to Bayesian Networks</a><li><span><i>2025-10-08  </i></span>&emsp;<a href="/blog/meta_learning/meta_learning/">Meta-Learning in Machine Learning</a><li><span><i>2025-10-08  </i></span>&emsp;<a href="/blog/meta_learning/metric_learning/">Metric-Based Meta-Learning: The Intuitive Approach</a><li><span><i>2025-10-06  </i></span>&emsp;<a href="/blog/fewshots/learn_tail/">Learning to model the tail</a><li><span><i>2025-10-06  </i></span>&emsp;<a href="/blog/hashing/multiply_shift_hash/">Multiply-Shift Hashing: Fast and Elegant</a><li><span><i>2025-10-06  </i></span>&emsp;<a href="/blog/hashing/why_hash/">Why Hashing is Great: An Intuitive Guide</a><li><span><i>2025-10-06  </i></span>&emsp;<a href="/blog/interpretation/shap/">SHAP and DeepLIFT: A Technical Summary</a><li><span><i>2025-10-06  </i></span>&emsp;<a href="/blog/machine-learning/gating_activation/">Self-gating activation functions</a></ul>
<div class="page-foot">
    Contact me by <a href="mailto:skchu@wustl.edu">E-mail</a> | <a href="https://github.com/kchu25">Github</a> | <a href="https://www.linkedin.com/in/kchu1/">Linkedin</a>
    <br>
    This work is licensed under <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a>.  Last modified: December 01, 2025.
    <br>
    Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia language</a>.
</div>
</div><!-- CONTENT ENDS HERE -->
    
    
  </body>
</html>
