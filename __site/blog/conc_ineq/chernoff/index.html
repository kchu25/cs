<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
   <link rel="stylesheet" href="/libs/katex/katex.min.css">
     
  
  <link rel="stylesheet" href="/css/franklin.css">
<link rel="stylesheet" href="/css/hypertext.css">
<link rel="icon" href="/assets/favicon.png">

   <title>Step-by-Step Guide: Using Chernoff Bounds</title>  
</head>
<body>
<header>
  <h1 style="color:#283747">computer science</h1>
  <nav>
    <a href="/" class="current">Tags</a>
  | <a href="/blog/" >Notes</a>
  <hr/>
  </nav>
</header>


<!-- Content appended here -->
<div class="franklin-content"><h1 id="step-by-step_guide_using_chernoff_bounds"><a href="#step-by-step_guide_using_chernoff_bounds" class="header-anchor">Step-by-Step Guide: Using Chernoff Bounds</a></h1>
<h2 id="whats_the_difference_chernoff_vs_hoeffding"><a href="#whats_the_difference_chernoff_vs_hoeffding" class="header-anchor">What&#39;s the difference: Chernoff vs Hoeffding?</a></h2>
<p><strong>The short answer:</strong> They&#39;re both concentration inequalities, but:</p>
<ul>
<li><p><strong>Chernoff bound</strong>: Specifically for <strong>Bernoulli</strong> random variables &#40;0/1, like coin flips&#41;, gives bounds using KL-divergence. Tighter when probabilities are small.</p>
</li>
<li><p><strong>Hoeffding bound</strong>: Works for <strong>any bounded</strong> random variables &#40;e.g., values in \([a,b]\)&#41;, uses simpler quadratic form. More general but sometimes looser.</p>
</li>
</ul>
<p><strong>In practice:</strong> People often say &quot;Chernoff bound&quot; colloquially to mean either one, since they&#39;re both exponential tail bounds derived using similar techniques &#40;moment generating functions&#41;. The page you linked uses them somewhat interchangeably.</p>
<h2 id="what_is_it"><a href="#what_is_it" class="header-anchor">What is it?</a></h2>
<p>These bounds tell you: <strong>&quot;If you sample from a random process, how likely is your average to be far from the true mean?&quot;</strong></p>
<p>Think of it as your statistical safety net when you can&#39;t check everything.</p>
<hr />
<h2 id="the_core_formulas"><a href="#the_core_formulas" class="header-anchor">The Core Formulas</a></h2>
<h3 id="hoeffdings_inequality_general_bounded_case"><a href="#hoeffdings_inequality_general_bounded_case" class="header-anchor">Hoeffding&#39;s Inequality &#40;General Bounded Case&#41;</a></h3>
<p>If you have \(n\) independent samples \(X_1, \ldots, X_n\) where each \(X_i \in [a,b]\), and you compute their average \(\bar{X} = \frac{1}{n}\sum X_i\), then:</p>
\[P(|\bar{X} - \mu| > t) \leq 2e^{-\frac{2nt^2}{(b-a)^2}}\]
<p>where \(\mu = E[\bar{X}]\) is the true mean, and \(t\) is your &quot;tolerance&quot; for error.</p>
<p><strong>Special case:</strong> When \(X_i \in [0,1]\), this simplifies to \(2e^{-2nt^2}\).</p>
<h3 id="chernoff_bound_bernoulli_case"><a href="#chernoff_bound_bernoulli_case" class="header-anchor">Chernoff Bound &#40;Bernoulli Case&#41;</a></h3>
<p>For Bernoulli variables &#40;coin flips&#41; with \(X = \sum_{i=1}^n X_i\) where \(E[X] = \mu\):</p>
\[P(X \geq (1+\delta)\mu) \leq e^{-\frac{\delta^2 \mu}{3}} \quad \mathrm{ for} 0 < \delta \leq 1\]
\[P(X \geq (1+\delta)\mu) \leq e^{-\delta\mu/3} \quad \mathrm{ for} \delta > 1\]
<p>This is <strong>tighter</strong> when \(\mu\) is small &#40;rare events&#41;. Uses relative deviation \((1+\delta)\) rather than absolute deviation.</p>
<blockquote>
<p><strong>Where&#39;s the KL-divergence?</strong></p>
<p>The exact Chernoff bound is actually:</p>
</blockquote>
\[P(X \geq (1+\delta)\mu) \leq e^{-n \cdot D_{KL}(p(1+\delta) \,||\, p)}\]
<blockquote>
<p>where \(p = \mu/n\) is the success probability per trial, and \(D_{KL}\) is the KL-divergence between two Bernoulli distributions with probabilities \(p(1+\delta)\) and \(p\).</p>
<p>The KL-divergence for Bernoulli is: \(D_{KL}(q||p) = q\ln(q/p) + (1-q)\ln((1-q)/(1-p))\)</p>
<p>The formulas \(e^{-\delta^2\mu/3}\) and \(e^{-\delta\mu/3}\) above are <strong>approximations</strong> of this KL-divergence term that are easier to work with. They come from Taylor expanding the KL-divergence for small/large \(\delta\).</p>
</blockquote>
<blockquote>
<p><strong>Does \(\delta\) need to be between 0 and 1?</strong></p>
<p><strong>Short answer:</strong> No, \(\delta\) doesn&#39;t <em>need</em> to be between 0 and 1. That&#39;s just where the bound has its nicest, simplest form.</p>
<p><strong>What&#39;s really going on:</strong></p>
<p>The Chernoff bound works for <strong>any</strong> \(\delta > 0\). The restriction \(0 < \delta \leq 1\) is just when you get that clean \(e^{-\delta^2\mu/3}\) formula. Think of it like this:</p>
<ul>
<li><p>For <strong>small deviations</strong> &#40;\(\delta \leq 1\)&#41;: The bound is approximately \(e^{-\delta^2\mu/3}\)—that&#39;s the &quot;squared&quot; version that&#39;s easiest to work with</p>
</li>
<li><p>For <strong>large deviations</strong> &#40;\(\delta > 1\)&#41;: The bound becomes roughly \(e^{-\delta\mu/3}\)—still exponential, just linear in \(\delta\) instead of quadratic</p>
</li>
</ul>
<p><strong>Why split it up?</strong> Because the exact Chernoff bound is \(\left(\frac{e^\delta}{(1+\delta)^{1+\delta}}\right)^\mu\), which is messy. When you simplify this for small \(\delta\), you get the \(\delta^2\) version. For large \(\delta\), you get a different simplification.</p>
<p><strong>In practice:</strong> Most people only care about \(\delta \leq 1\) because you&#39;re typically looking for modest deviations &#40;like &quot;within 10&#37; or 50&#37;&quot;&#41;, not &quot;10× the mean.&quot; But mathematically, the bound is valid everywhere—just with different forms.</p>
</blockquote>
<p><strong>Translation:</strong> The probability your sample is off drops <em>exponentially</em> fast as you take more samples.</p>
<hr />
<h2 id="step-by-step_how_to_use_it"><a href="#step-by-step_how_to_use_it" class="header-anchor">Step-by-Step: How to Use It</a></h2>
<h3 id="step_1_figure_out_what_youre_measuring"><a href="#step_1_figure_out_what_youre_measuring" class="header-anchor">Step 1: Figure out what you&#39;re measuring</a></h3>
<p>Ask yourself:</p>
<ul>
<li><p>What&#39;s my random variable? &#40;coin flips? click-through rates? test scores?&#41;</p>
</li>
<li><p>What&#39;s the true mean \(\mu\) I&#39;m trying to estimate?</p>
</li>
<li><p>Each sample should be independent and bounded &#40;usually between 0 and 1&#41;</p>
</li>
</ul>
<p><strong>Example:</strong> You&#39;re A/B testing a website. Each visitor either clicks &#40;1&#41; or doesn&#39;t &#40;0&#41;. True click rate is \(\mu = 0.05\) &#40;5&#37;&#41;.</p>
<hr />
<h3 id="step_2_decide_your_tolerance_and_confidence"><a href="#step_2_decide_your_tolerance_and_confidence" class="header-anchor">Step 2: Decide your tolerance and confidence</a></h3>
<p>Pick two numbers:</p>
<ul>
<li><p><strong>Tolerance \(t\):</strong> How accurate do you want to be? </p>
<ul>
<li><p>E.g., &quot;I want my estimate within 1&#37; of the true value&quot; → \(t = 0.01\)</p>
</li>
</ul>
</li>
<li><p><strong>Confidence \(\delta\):</strong> How sure do you want to be?</p>
<ul>
<li><p>E.g., &quot;I want to be 95&#37; confident&quot; → \(\delta = 0.05\) &#40;failure probability&#41;</p>
</li>
</ul>
</li>
</ul>
<hr />
<h3 id="step_3_solve_for_sample_size_n"><a href="#step_3_solve_for_sample_size_n" class="header-anchor">Step 3: Solve for sample size \(n\)</a></h3>
<p><strong>Using Hoeffding:</strong> Set the bound equal to your confidence level:</p>
\[2e^{-2nt^2} = \delta\]
<p>Solve for \(n\):</p>
\[n = \frac{\ln(2/\delta)}{2t^2}\]
<p><strong>Example:</strong> For \(t = 0.01\) and \(\delta = 0.05\):</p>
\[n = \frac{\ln(2/0.05)}{2(0.01)^2} = \frac{\ln(40)}{0.0002} = \frac{3.69}{0.0002} \approx 18{,}450\]
<p>You need about <strong>18,450 samples</strong> to estimate a rate within 1&#37; with 95&#37; confidence.</p>
<p><strong>When to use Chernoff instead:</strong> If you know your random variables are Bernoulli &#40;0/1&#41; and the probability \(p\) is small, use the Chernoff bound with relative error. It&#39;ll give you tighter &#40;fewer samples needed&#41; results.</p>
<hr />
<h3 id="step_4_collect_your_samples"><a href="#step_4_collect_your_samples" class="header-anchor">Step 4: Collect your samples</a></h3>
<p>Run your experiment and collect \(n\) independent samples. Compute the average:</p>
\[\bar{X} = \frac{1}{n}\sum_{i=1}^n X_i\]
<p><strong>Example:</strong> After 18,450 visitors, 932 clicked. Your estimate: \(\bar{X} = 932/18450 \approx 0.0505\) &#40;5.05&#37;&#41;.</p>
<hr />
<h3 id="step_5_report_with_confidence"><a href="#step_5_report_with_confidence" class="header-anchor">Step 5: Report with confidence</a></h3>
<p>You can now say: <strong>&quot;With 95&#37; confidence, the true click rate is between 4.05&#37; and 6.05&#37;&quot;</strong></p>
<p>The bound guarantees that \(P(|\bar{X} - \mu| > 0.01) \leq 0.05\).</p>
<hr />
<h2 id="derivation_where_do_these_sample_complexity_formulas_come_from"><a href="#derivation_where_do_these_sample_complexity_formulas_come_from" class="header-anchor">Derivation: Where Do These Sample Complexity Formulas Come From?</a></h2>
<p>You might be wondering: &quot;How did we get \(n \geq \frac{\ln(2/\delta)}{2t^2}\)?&quot; Here&#39;s the step-by-step for each pattern.</p>
<h3 id="pattern_a_find_at_least_one_good_item_neither_boundbasic_probability"><a href="#pattern_a_find_at_least_one_good_item_neither_boundbasic_probability" class="header-anchor">Pattern A: Find at least one good item &#40;Neither bound—basic probability&#33;&#41;</a></h3>
<p><strong>Which bound?</strong> Actually, this doesn&#39;t use Chernoff or Hoeffding&#33; It&#39;s just basic probability.</p>
<p><strong>Setup:</strong> Probability of success per sample is \(\epsilon\). After \(n\) samples, probability of getting zero successes is:</p>
\[P(\mathrm{ all fail}) = (1-\epsilon)^n\]
<p><strong>Goal:</strong> Make this failure probability ≤ \(\delta\):</p>
\[(1-\epsilon)^n \leq \delta\]
<p><strong>Key trick:</strong> Use the approximation \((1-\epsilon)^n \approx e^{-n\epsilon}\) &#40;valid for small \(\epsilon\)&#41;:</p>
\(e^{-n\epsilon} \leq \delta\)
<blockquote>
<p><strong>Why is \((1-\epsilon)^n \leq e^{-n\epsilon}\)?</strong></p>
<p>This is actually an inequality, not just an approximation, and <strong>yes, convexity is involved&#33;</strong></p>
<p><strong>The key inequality:</strong> \(1 - x \leq e^{-x}\) for all \(x \geq 0\)</p>
<p>You can see this by looking at the functions: \(e^{-x}\) curves upward &#40;convex&#41;, while \(1-x\) is a straight line. The exponential always stays above the line.</p>
<p><strong>Apply it \(n\) times:</strong> If \(1 - \epsilon \leq e^{-\epsilon}\), then raising both sides to the \(n\)-th power:</p>
\((1-\epsilon)^n \leq (e^{-\epsilon})^n = e^{-n\epsilon}\)
<p><strong>Practically:</strong> For small \(\epsilon\), they&#39;re nearly equal. For example, \((1-0.01)^{100} = 0.366\) while \(e^{-1} = 0.368\). But mathematically, we use the inequality \((1-\epsilon)^n \leq e^{-n\epsilon}\) to get our bound, and this holds for any \(\epsilon \in [0,1]\).</p>
</blockquote>
<p><strong>Solve for \(n\):</strong> Take natural log of both sides:</p>
\[-n\epsilon \leq \ln(\delta)\]
\[n\epsilon \geq -\ln(\delta) = \ln(1/\delta)\]
\[n \geq \frac{\ln(1/\delta)}{\epsilon}\]
<p>Done&#33; This is why we only need \(O(1/\epsilon)\) samples—no \(\epsilon^2\) here. We&#39;re just asking &quot;did we see at least one?&quot; not &quot;how accurate is our estimate?&quot;</p>
<hr />
<h3 id="pattern_a_extended_estimate_mean_within_pm_t_uses_hoeffding"><a href="#pattern_a_extended_estimate_mean_within_pm_t_uses_hoeffding" class="header-anchor">Pattern A &#40;extended&#41;: Estimate mean within \(\pm t\) &#40;Uses Hoeffding&#41;</a></h3>
<p><strong>Which bound?</strong> <strong>Hoeffding&#39;s inequality</strong></p>
<p><strong>Setup:</strong> Start with Hoeffding&#39;s inequality:</p>
\[P(|\bar{X} - \mu| > t) \leq 2e^{-2nt^2}\]
<p><strong>Goal:</strong> Make this failure probability ≤ \(\delta\):</p>
\[2e^{-2nt^2} \leq \delta\]
<p><strong>Solve for \(n\):</strong> </p>
\[e^{-2nt^2} \leq \delta/2\]
<p>Take natural log:</p>
\[-2nt^2 \leq \ln(\delta/2)\]
\[2nt^2 \geq -\ln(\delta/2) = \ln(2/\delta)\]
\[n \geq \frac{\ln(2/\delta)}{2t^2}\]
<p>The \(t^2\) in the denominator is the key—this is why estimation needs more samples than detection&#33;</p>
<hr />
<h3 id="pattern_a_chernoff_version_estimate_bernoulli_mean_with_relative_error"><a href="#pattern_a_chernoff_version_estimate_bernoulli_mean_with_relative_error" class="header-anchor">Pattern A &#40;Chernoff version&#41;: Estimate Bernoulli mean with relative error</a></h3>
<p><strong>Which bound?</strong> <strong>Chernoff bound</strong></p>
<p><strong>Setup:</strong> For Bernoulli with \(X = \sum_{i=1}^n X_i\) and \(E[X] = np\) &#40;where \(p\) is success probability&#41;, Chernoff gives:</p>
\(P(X \geq (1+\delta)np) \leq e^{-\frac{\delta^2 np}{3}}\)
<p>For two-sided &#40;both upper and lower deviation&#41;, we get:</p>
\(P(|X - np| > \delta \cdot np) \leq 2e^{-\frac{\delta^2 np}{3}}\)
<p><strong>Goal:</strong> We want the empirical frequency \(\hat{p} = X/n\) to be within relative error \(\delta\) of true \(p\). Set failure probability ≤ \(\delta_{conf}\):</p>
\(2e^{-\frac{\delta^2 np}{3}} \leq \delta_{conf}\)
<p><strong>Solve for \(n\):</strong></p>
\(e^{-\frac{\delta^2 np}{3}} \leq \frac{\delta_{conf}}{2}\)
<p>Take natural log:</p>
<p>\(-\frac{\delta^2 np}{3} \leq \ln(\delta_{conf}/2)\) \(\frac{\delta^2 np}{3} \geq \ln(2/\delta_{conf})\) \(n \geq \frac{3\ln(2/\delta_{conf})}{\delta^2 p}\)</p>
<p><strong>Key difference from Hoeffding:</strong> </p>
<ul>
<li><p>Chernoff has \(p\) in denominator &#40;works better when \(p\) is small&#33;&#41;</p>
</li>
<li><p>Uses <strong>relative</strong> error \(\delta\) &#40;like &quot;within 20&#37;&quot;&#41; instead of <strong>absolute</strong> error \(t\)</p>
</li>
<li><p>For small \(p\) &#40;rare events&#41;, this gives tighter bounds than Hoeffding</p>
</li>
</ul>
<hr />
<h3 id="theorem_template_pattern_a_chernoff"><a href="#theorem_template_pattern_a_chernoff" class="header-anchor">Theorem Template: Pattern A &#40;Chernoff&#41;</a></h3>
<blockquote>
<p><strong>Theorem &#40;Bernoulli Frequency Estimation - Chernoff Bound&#41;</strong></p>
<p>Let \(X_1, \ldots, X_n\) be i.i.d. Bernoulli random variables with success probability \(p\). Define the empirical frequency \(\hat{p} = \frac{1}{n}\sum_{i=1}^n X_i\).</p>
<p><strong>Given:</strong></p>
<ul>
<li><p>Target probability \(p\) &#40;or lower bound \(p_{min}\) if unknown&#41;</p>
</li>
<li><p>Relative error tolerance \(\delta \in (0, 1]\) &#40;e.g., \(\delta = 0.2\) for 20&#37; relative error&#41;</p>
</li>
<li><p>Confidence level \(1 - \delta_{conf}\) &#40;e.g., \(\delta_{conf} = 0.05\) for 95&#37; confidence&#41;</p>
</li>
</ul>
<p><strong>Guarantee:</strong> If we take</p>
\(n \geq \frac{3\ln(2/\delta_{conf})}{\delta^2 p}\)
<p>samples, then with probability at least \(1 - \delta_{conf}\):</p>
\(|\hat{p} - p| \leq \delta \cdot p\)
<p><strong>In words:</strong> Our estimate \(\hat{p}\) is within \(\delta\)<em>relative</em> to the true probability \(p\).</p>
<p><strong>Example:</strong> For \(p = 0.02\), \(\delta = 0.5\) &#40;50&#37; relative error&#41;, \(\delta_{conf} = 0.05\) &#40;95&#37; confidence&#41;:</p>
<ul>
<li><p>Sample size: \(n \geq \frac{3\ln(40)}{0.25 \times 0.02} \approx 2{,}214\)</p>
</li>
<li><p>Guarantee: With 95&#37; confidence, \(\hat{p} \in [0.01, 0.03]\) &#40;i.e., between 1&#37; and 3&#37;&#41;</p>
</li>
</ul>
</blockquote>
<hr />
<h3 id="pattern_b_c_multiple_testing_with_union_bound_uses_hoeffding"><a href="#pattern_b_c_multiple_testing_with_union_bound_uses_hoeffding" class="header-anchor">Pattern B &amp; C: Multiple testing with union bound &#40;Uses Hoeffding&#41;</a></h3>
<p><strong>Which bound?</strong> <strong>Hoeffding&#39;s inequality &#43; union bound</strong></p>
<p><strong>Setup:</strong> We have \(M\) different modes/items to test. For each mode \(i\), Hoeffding gives:</p>
\[P(|\bar{X}_i - \mu_i| > \epsilon) \leq 2e^{-2n\epsilon^2}\]
<p><strong>Goal:</strong> We want <em>all</em> \(M\) estimates to be accurate. The probability that <em>at least one</em> fails is:</p>
\[P(\mathrm{ any fails}) \leq \sum_{i=1}^M P(|\bar{X}_i - \mu_i| > \epsilon) \leq M \cdot 2e^{-2n\epsilon^2}\]
<p>This is the <strong>union bound</strong>—we just added up all individual failure probabilities.</p>
<p><strong>Set this ≤ \(\delta\):</strong></p>
\[M \cdot 2e^{-2n\epsilon^2} \leq \delta\]
<p><strong>Solve for \(n\):</strong></p>
\[e^{-2n\epsilon^2} \leq \frac{\delta}{2M}\]
<p>Take natural log:</p>
\[-2n\epsilon^2 \leq \ln\left(\frac{\delta}{2M}\right)\]
\[2n\epsilon^2 \geq -\ln\left(\frac{\delta}{2M}\right) = \ln\left(\frac{2M}{\delta}\right)\]
\[n \geq \frac{\ln(2M/\delta)}{2\epsilon^2}\]
<p>Often simplified to \(n \geq \frac{\ln(M/\delta)}{2\epsilon^2}\) by absorbing the constant 2.</p>
<p><strong>Key insight:</strong> The \(M\) shows up inside the logarithm &#40;from union bound&#41;, while \(\epsilon^2\) is in the denominator &#40;from Hoeffding&#41;. That&#39;s why we get \(\ln M\) growth—logarithms grow incredibly slowly&#33;</p>
<hr />
<h3 id="pattern_b_c_chernoff_version_multiple_bernoulli_tests"><a href="#pattern_b_c_chernoff_version_multiple_bernoulli_tests" class="header-anchor">Pattern B &amp; C &#40;Chernoff version&#41;: Multiple Bernoulli tests</a></h3>
<p><strong>Which bound?</strong> <strong>Chernoff bound &#43; union bound</strong></p>
<p><strong>Setup:</strong> For \(M\) different Bernoulli modes with probabilities \(p_1, \ldots, p_M\). For each mode \(i\):</p>
\[P(|\hat{p}_i - p_i| > \delta \cdot p_i) \leq 2e^{-\frac{\delta^2 np_i}{3}}\]
<p><strong>Union bound over all \(M\) modes:</strong></p>
\[P(\mathrm{ any fails}) \leq M \cdot 2e^{-\frac{\delta^2 n p_{min}}{3}}\]
<p>where \(p_{min}\) is the smallest probability we care about.</p>
<p><strong>Set this ≤ \(\delta_{conf}\):</strong></p>
\[M \cdot 2e^{-\frac{\delta^2 n p_{min}}{3}} \leq \delta_{conf}\]
<p><strong>Solve for \(n\):</strong></p>
\[n \geq \frac{3\ln(2M/\delta_{conf})}{\delta^2 p_{min}}\]
<p><strong>When Chernoff is better:</strong> If \(p_{min}\) is small &#40;say 0.01&#41;, Chernoff needs fewer samples than Hoeffding for the same relative accuracy.</p>
<hr />
<h3 id="summary_of_derivations"><a href="#summary_of_derivations" class="header-anchor">Summary of Derivations</a></h3>
<table><tr><th align="right">Pattern</th><th align="right">Bound Used</th><th align="right">Starting Inequality</th><th align="right">Result</th></tr><tr><td align="right">Find ≥1 item</td><td align="right">Basic probability</td><td align="right">\((1-\epsilon)^n \approx e^{-n\epsilon}\)</td><td align="right">\(n \geq \frac{\ln(1/\delta)}{\epsilon}\)</td></tr><tr><td align="right">Estimate &#40;absolute error&#41;</td><td align="right"><strong>Hoeffding</strong></td><td align="right">\(2e^{-2nt^2} \leq \delta\)</td><td align="right">\(n \geq \frac{\ln(2/\delta)}{2t^2}\)</td></tr><tr><td align="right">Estimate &#40;relative error&#41;</td><td align="right"><strong>Chernoff</strong></td><td align="right">\(2e^{-\frac{\delta^2 np}{3}} \leq \delta_{conf}\)</td><td align="right">\(n \geq \frac{3\ln(2/\delta_{conf})}{\delta^2 p}\)</td></tr><tr><td align="right">\(M\) items &#40;absolute&#41;</td><td align="right"><strong>Hoeffding &#43; union</strong></td><td align="right">\(M \cdot 2e^{-2n\epsilon^2} \leq \delta\)</td><td align="right">\(n \geq \frac{\ln(M/\delta)}{2\epsilon^2}\)</td></tr><tr><td align="right">\(M\) items &#40;relative&#41;</td><td align="right"><strong>Chernoff &#43; union</strong></td><td align="right">\(M \cdot 2e^{-\frac{\delta^2 np}{3}} \leq \delta_{conf}\)</td><td align="right">\(n \geq \frac{3\ln(2M/\delta_{conf})}{\delta^2 p}\)</td></tr></table>
<p>The common thread: <strong>Start with exponential tail bound → Set ≤ \(\delta\) → Take log → Solve for \(n\)</strong></p>
<p><strong>Key takeaway:</strong> Hoeffding uses absolute error and works for any bounded variables. Chernoff uses relative error and works specifically for Bernoulli, giving tighter bounds when probabilities are small.</p>
<hr />
<h2 id="two_common_patterns"><a href="#two_common_patterns" class="header-anchor">Two Common Patterns</a></h2>
<h3 id="pattern_a_find_at_least_one_good_thing"><a href="#pattern_a_find_at_least_one_good_thing" class="header-anchor">Pattern A: &quot;Find at least one good thing&quot;</a></h3>
<p><strong>Question:</strong> You have a pile of \(N\) items, \(\epsilon\) fraction are good. How many samples to find <em>one</em> good item?</p>
<p><strong>Answer:</strong> Much easier&#33; You only need:</p>
\[n \geq \frac{\ln(1/\delta)}{\epsilon}\]
<p><strong>Why?</strong> You&#39;re not estimating a frequency—you just need \(X \geq 1\). Probability of missing all: \((1-\epsilon)^n \approx e^{-n\epsilon}\). Set this equal to \(\delta\) and solve.</p>
<p><strong>Example:</strong> If 1&#37; are good &#40;\(\epsilon = 0.01\)&#41; and you want 99&#37; confidence &#40;\(\delta = 0.01\)&#41;:</p>
\[n = \frac{\ln(100)}{0.01} = \frac{4.6}{0.01} = 460\]
<p>Just <strong>460 samples</strong>&#33; Notice: no \(\epsilon^2\), and no dependence on \(N\).</p>
<hr />
<h3 id="pattern_b_avoid_false_alarms_across_many_items"><a href="#pattern_b_avoid_false_alarms_across_many_items" class="header-anchor">Pattern B: &quot;Avoid false alarms across many items&quot;</a></h3>
<p><strong>Question:</strong> You&#39;re testing \(N\) different hypotheses. How many samples per item to avoid false positives?</p>
<p><strong>Answer:</strong> Need to account for multiple testing &#40;union bound&#41;:</p>
\[n \geq \frac{\ln(N/\delta)}{2\epsilon^2}\]
<p><strong>Why?</strong> You&#39;re now making \(N\) different statistical tests. The \(\ln N\) appears from the union bound over all tests.</p>
<blockquote>
<p><strong>What&#39;s a &quot;union bound&quot;?</strong></p>
<p>Simple idea: If you&#39;re checking \(N\) different things, the chance that <em>at least one</em> goes wrong is at most the sum of individual failure probabilities.</p>
<p><strong>Example:</strong> You flip 100 coins. Each has 1&#37; chance of landing on its edge &#40;rare event&#41;. What&#39;s the chance <em>any</em> coin lands on edge?</p>
<ul>
<li><p>Naive: &quot;Probably pretty high since there are 100 coins&#33;&quot;</p>
</li>
<li><p>Union bound: &quot;At most \(100 \times 0.01 = 1\) &#40;i.e., 100&#37;&#41;&quot;—we add up all the individual 1&#37; chances</p>
</li>
</ul>
<p>In our case: We want <em>all</em> \(N\) estimates to be accurate. So we need each individual estimate to have failure probability \(\delta/N\). Then the total failure probability is at most \(N \times (\delta/N) = \delta\). That&#39;s where \(\ln(N/\delta)\) comes from—we&#39;re being careful about all \(N\) tests simultaneously.</p>
</blockquote>
<p><strong>Example:</strong> Testing \(N = 10{,}000\) items, want 99&#37; confidence overall &#40;\(\delta = 0.01\)&#41;, tolerance \(\epsilon = 0.05\):</p>
\[n = \frac{\ln(10000/0.01)}{2(0.05)^2} = \frac{\ln(10^6)}{0.005} = \frac{13.8}{0.005} \approx 2{,}760\]
<p>Need <strong>2,760 samples per item</strong>. Notice: \(\ln N\) grows slowly &#40;doubling \(N\) only adds \(\ln 2 \approx 0.7\) to the log&#41;.</p>
<hr />
<h3 id="pattern_c_discover_all_rare_modes_at_once"><a href="#pattern_c_discover_all_rare_modes_at_once" class="header-anchor">Pattern C: &quot;Discover all rare modes at once&quot;</a></h3>
<p><strong>Question:</strong> You have \(M\) different &quot;modes&quot; &#40;categories, items, etc.&#41;, each appearing with some small frequency. Can you sample once and discover all their frequencies?</p>
<p><strong>Answer:</strong> Yes&#33; That&#39;s the power of these bounds.</p>
\[n \geq \frac{\ln(M/\delta)}{2\epsilon^2}\]
<blockquote>
<p><strong>The magic of &quot;sample once, know forever&quot;:</strong></p>
<p>If you have multiple rare modes &#40;say, frequencies \(p_1 = 0.01\), \(p_2 = 0.03\), \(p_3 = 0.005\)&#41;, you can:</p>
<ol>
<li><p><strong>Sample once</strong> with the formula above</p>
</li>
<li><p><strong>Store all the results</strong> &#40;which modes appeared how often&#41;</p>
</li>
<li><p><strong>Get guarantees for ALL modes simultaneously</strong></p>
</li>
</ol>
<p><strong>Why this works:</strong></p>
<ul>
<li><p>Each sample is like rolling a giant die with \(M\) faces</p>
</li>
<li><p>The bound tells you: after \(n\) rolls, the frequency of <em>each face</em> will be close to its true probability</p>
</li>
<li><p>The \(\ln M\) factor accounts for testing all modes at once &#40;that&#39;s the union bound again&#33;&#41;</p>
</li>
<li><p>Since \(\ln M\) grows slowly &#40;even \(M = 1{,}000{,}000\) only adds ~14&#41;, you need way fewer samples than checking exhaustively</p>
</li>
</ul>
<p><strong>Example:</strong> 1 million possible modes, want to find all modes with frequency ≥ 0.01, with 95&#37; confidence &#40;\(\delta = 0.05\)&#41;:</p>
</blockquote>
\[n \approx \frac{\ln(10^6/0.05)}{2(0.01)^2} \approx \frac{17}{0.0002} \approx 85{,}000 \mathrm{ samples}\]
<blockquote>
<p>That&#39;s it&#33; <strong>85k samples to characterize all 1 million modes.</strong> Sample once, know forever &#40;assuming the distribution doesn&#39;t change&#41;.</p>
<p>This is why random sampling is so powerful for discovering patterns in huge spaces—you don&#39;t need to check everything, just enough to be statistically confident about all of them simultaneously.</p>
</blockquote>
<hr />
<h2 id="the_magic_why_this_works"><a href="#the_magic_why_this_works" class="header-anchor">The Magic: Why This Works</a></h2>
<p>The key insight is <strong>exponential concentration</strong>. When you have independent samples, the probability of being far from the mean drops like \(e^{-n}\). </p>
<p>This is <em>way</em> faster than linear&#33; Even if you have a billion items &#40;\(N = 10^9\)&#41;, you only need \(\sim \ln(10^9) \approx 21\) in the exponent. That&#39;s why sampling beats exhaustive search.</p>
<p><strong>Intuition:</strong> Imagine flipping a coin 1000 times. You expect about 500 heads. What&#39;s the chance you get 600? It&#39;s astronomically small—like \(10^{-20}\). That&#39;s concentration at work.</p>
<hr />
<h2 id="when_should_i_use_hoeffding_vs_chernoff"><a href="#when_should_i_use_hoeffding_vs_chernoff" class="header-anchor">When Should I Use Hoeffding vs Chernoff?</a></h2>
<p>Here&#39;s a practical decision tree:</p>
<h3 id="use_hoeffding_when"><a href="#use_hoeffding_when" class="header-anchor">Use <strong>Hoeffding</strong> when:</a></h3>
<ul>
<li><p>✅ Your data can take <strong>any values</strong> in a range &#40;not just 0/1&#41;</p>
<ul>
<li><p><em>Example: Survey responses on a 1-10 scale, test scores, temperatures</em></p>
</li>
</ul>
</li>
<li><p>✅ You want to think in terms of <strong>absolute error</strong> &#40;&quot;within ±0.05&quot;&#41;</p>
</li>
<li><p>✅ You want <strong>one formula that works everywhere</strong> &#40;simpler, more robust&#41;</p>
</li>
<li><p>✅ You&#39;re not sure about the underlying distribution</p>
</li>
</ul>
<p><strong>Hoeffding is the &quot;safe default&quot;</strong>—it always works for bounded variables.</p>
<hr />
<h3 id="use_chernoff_when"><a href="#use_chernoff_when" class="header-anchor">Use <strong>Chernoff</strong> when:</a></h3>
<ul>
<li><p>✅ Your data is <strong>binary</strong> &#40;0/1, success/failure, clicked/didn&#39;t click&#41;</p>
</li>
<li><p>✅ The probability of success is <strong>small</strong> &#40;\(p \ll 0.5\)&#41;</p>
<ul>
<li><p><em>Example: Rare disease detection, click-through rates, defect rates</em></p>
</li>
</ul>
</li>
<li><p>✅ You want to think in terms of <strong>relative error</strong> &#40;&quot;within 20&#37; of the true rate&quot;&#41;</p>
</li>
<li><p>✅ You want the <strong>tightest possible bound</strong> for your specific case</p>
</li>
</ul>
<p><strong>Chernoff gives you extra juice when dealing with rare events.</strong></p>
<hr />
<h3 id="quick_example_how_much_tighter_is_chernoff"><a href="#quick_example_how_much_tighter_is_chernoff" class="header-anchor">Quick Example: How Much Tighter is Chernoff?</a></h3>
<p>Say you&#39;re estimating a click-through rate of 2&#37; &#40;p &#61; 0.02&#41;:</p>
<p><strong>Hoeffding approach:</strong> &quot;I want my estimate within ±0.01 &#40;absolute&#41;&quot;</p>
<ul>
<li><p>Need \(n \approx \frac{\ln(2/\delta)}{2(0.01)^2} = \frac{\ln(40)}{0.0002} \approx 18{,}450\) samples</p>
</li>
</ul>
<p><strong>Chernoff approach:</strong> &quot;I want my estimate within 50&#37; relative error&quot; &#40;so between 1&#37; and 3&#37;&#41;</p>
<ul>
<li><p>Here \(\delta = 0.5\) &#40;relative error&#41;, \(p = 0.02\), \(\delta_{conf} = 0.05\)</p>
</li>
<li><p>Need \(n \approx \frac{3\ln(2/0.05)}{(0.5)^2 \times 0.02} = \frac{3 \times 3.69}{0.005} \approx 2{,}214\) samples</p>
</li>
</ul>
<p><strong>Result:</strong> Chernoff needs <strong>2,214 samples</strong> vs Hoeffding&#39;s <strong>18,450 samples</strong>—that&#39;s <strong>8.3× fewer samples&#33;</strong> </p>
<blockquote>
<p><strong>Why such a huge difference?</strong></p>
<ul>
<li><p>Hoeffding: \(n \propto \frac{1}{t^2}\) where \(t = 0.01\) &#40;absolute tolerance&#41;</p>
</li>
<li><p>Chernoff: \(n \propto \frac{1}{\delta^2 p}\) where \(\delta = 0.5\), \(p = 0.02\)</p>
</li>
</ul>
<p><strong>The key:</strong> Chernoff has that \(p\) in the denominator. When \(p\) is small &#40;rare events&#41;, the sample complexity goes <em>down</em>. Hoeffding doesn&#39;t know about \(p\), so it has to be conservative for all possible probabilities.</p>
<p><strong>The tradeoff:</strong> Chernoff requires you to think in relative terms &#40;&quot;within 50&#37; of truth&quot;&#41; and you need to know roughly what \(p\) is beforehand. Hoeffding just asks for absolute accuracy, no prior knowledge needed.</p>
<p><strong>Bottom line:</strong> For rare events &#40;\(p < 0.1\)&#41;, Chernoff can be <strong>5-10× more sample-efficient</strong> than Hoeffding&#33;</p>
</blockquote>
<hr />
<h3 id="the_rule_of_thumb"><a href="#the_rule_of_thumb" class="header-anchor">The Rule of Thumb</a></h3>
<ul>
<li><p><strong>General bounded data or don&#39;t know the distribution?</strong> → Use <strong>Hoeffding</strong></p>
</li>
<li><p><strong>Binary data with small probabilities?</strong> → Use <strong>Chernoff</strong> &#40;you&#39;ll get tighter bounds&#41;</p>
</li>
<li><p><strong>Binary data with probabilities near 0.5?</strong> → Both work similarly, use <strong>Hoeffding</strong> &#40;simpler&#41;</p>
</li>
</ul>
<hr />
<h2 id="quick_reference_table"><a href="#quick_reference_table" class="header-anchor">Quick Reference Table</a></h2>
<table><tr><th align="right"><strong>Goal</strong></th><th align="right"><strong>Formula</strong></th><th align="right"><strong>Key</strong></th><th align="right"><strong>Best for</strong></th></tr><tr><td align="right">Estimate mean within \(\pm t\) &#40;Hoeffding&#41;</td><td align="right">\(n \geq \frac{\ln(2/\delta)}{2t^2}\)</td><td align="right">Need \(t^2\) for accuracy</td><td align="right">Any bounded variables</td></tr><tr><td align="right">Estimate mean &#40;Chernoff, relative error&#41;</td><td align="right">\(n \geq \frac{3\ln(2/\delta_{conf})}{\delta^2 p}\)</td><td align="right">Tighter for small \(p\)</td><td align="right">Rare events &#40;Bernoulli&#41;</td></tr><tr><td align="right">Find \(\geq 1\) good item</td><td align="right">\(n \geq \frac{\ln(1/\delta)}{\epsilon}\)</td><td align="right">Just need \(\epsilon\), much easier</td><td align="right">Detection problems</td></tr><tr><td align="right">Test \(M\) items, no false alarms</td><td align="right">\(n \geq \frac{\ln(M/\delta)}{2\epsilon^2}\)</td><td align="right">Union bound adds \(\ln M\)</td><td align="right">Multiple testing</td></tr></table>
<hr />
<h2 id="common_gotchas"><a href="#common_gotchas" class="header-anchor">Common Gotchas</a></h2>
<ol>
<li><p><strong>Independence matters</strong>: If your samples aren&#39;t independent, all bets are off</p>
</li>
<li><p><strong>The \(t^2\) hurts</strong>: To halve your error, you need 4× the samples</p>
</li>
<li><p><strong>Bounded range</strong>: Hoeffding assumes values in \([a,b]\). If not, rescale first</p>
</li>
<li><p><strong>One-sided vs two-sided</strong>: The formula above is two-sided &#40;\(|\bar{X} - \mu| > t\)&#41;. For one-sided &#40;just \(\bar{X} > \mu + t\)&#41;, drop the factor of 2</p>
</li>
<li><p><strong>Chernoff vs Hoeffding choice</strong>: </p>
<ul>
<li><p>Use <strong>Chernoff</strong> when you have Bernoulli variables and small probabilities &#40;gives tighter bounds&#41;</p>
</li>
<li><p>Use <strong>Hoeffding</strong> when you have general bounded variables or don&#39;t know the distribution well &#40;more robust&#41;</p>
</li>
</ul>
</li>
</ol>
<hr />
<h2 id="the_bottom_line"><a href="#the_bottom_line" class="header-anchor">The Bottom Line</a></h2>
<p>Chernoff/Hoeffding bounds are your tool for answering: <strong>&quot;How many samples do I need to be confident?&quot;</strong></p>
<p>The answer is usually <strong>surprisingly small</strong>—often logarithmic in the problem size—because of exponential concentration. That&#39;s what makes modern data science and machine learning possible: you don&#39;t need to see everything, just enough to be statistically confident.</p>
<div class="page-foot">
    Contact me by <a href="mailto:skchu@wustl.edu">E-mail</a> | <a href="https://github.com/kchu25">Github</a> | <a href="https://www.linkedin.com/in/kchu1/">Linkedin</a>
    <br>
    This work is licensed under <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a>.  Last modified: January 03, 2026.
    <br>
    Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia language</a>.
</div>
</div><!-- CONTENT ENDS HERE -->
    
        <script src="/libs/katex/katex.min.js"></script>
<script src="/libs/katex/contrib/auto-render.min.js"></script>
<script>renderMathInElement(document.body)</script>

    
    
  </body>
</html>
